{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "We'll start of simple, with a regression in which one variable X predicts another variable Y. This is roughly equivalent to a correlation. In order to make this a bit more exciting, let's look at some real data!\n",
    "\n",
    "### Background to our data\n",
    "\n",
    "At the Winter Olympics, there is a sport called *speed skating*. If you're not from a select few countries, you might not have heard of it. It's essentially like running, but then you do it on ice, using special skates. In most disciplines, only two skaters compete against each other at a time. The times are compared between all athletes afterwards, and whoever had the best time wins. One of the more exciting races, is the 500 meter sprint. All competing athletes are *highly* trained, and focus on raw power and technique, as that is all that matters. Right?\n",
    "\n",
    "Well, perhaps not. Perhaps coincidence or even bias might also play a role. You see, the starting procedure in speed skating is like most other racing sports. The referee says \"Ready?\", then waits for a bit, and then shoots their starting pistol. The regulations are clear on that \"bit\" of time: It should be 1-1.5 seconds from the moment athletes are in position, and hence it's regulated to be random.\n",
    "\n",
    "Unlike in other racing sports, speed skaters race alone or against a single opponent, and times are compared between all skaters afterwards. (So the main competitors for the win might not directly face each other!) This means that every pair of racers starts with a different interval between their \"Ready\" cue and the starting shot.\n",
    "\n",
    "Does this matter? In theory, the \"Ready\" signal could be considered an *alerting cue*, and we know from psychological research that the time between an alerting cue and a subsequent signal affects how quickly people respond to that signal. In practice, an interval of 500 ms results in an optimal response time, and longer intervals result in higher response times.\n",
    "\n",
    "Now, obviously, the Winter Olympics are not some psychological experiment. These aren't a bunch of students in some dank lab room, these are highly trained athletes competing under immense pressure, looked on by thousands of spectators in the stadium, and even more watching from home through live stream or television. Surely this alerting thing will not affect their actual performance?\n",
    "\n",
    "That sounds like an empirical question!\n",
    "\n",
    "### Loading our data\n",
    "\n",
    "The National Skating Union records not only finish times at 500 meters, but also measures athletes' time 100 meters into the race. Both of these numbers are freely available. Researchers from the Universities of Oxford (UK) and Utrecht (Netherlands) have collected data on the intervals between the onset of the \"Ready\" signal and the onset of the starting shot. The attached file, `speed_skating_all_races.csv`, has both the 100 and 500 meter times, and the ready-start intervals. In addition, there is a column that indicates whether an athlete fell or stumbled during a race.\n",
    "\n",
    "You can load those data into Python using NumPy's `loadtxt` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Load the data from all individual races.\n",
    "data = numpy.loadtxt(\"speed_skating_all_races.csv\", \\\n",
    "    delimiter=\",\", dtype=float, skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some convenience renaming on the columns in the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ready-start interval data.\n",
    "interval = data[0,:]\n",
    "# Get the times at 100 and 500 meters.\n",
    "time_100m = data[1,:]\n",
    "time_500m = data[2,:]\n",
    "# Get the sex data, which is formatted as a 1 for male and\n",
    "# 0 for female. We can cast this into Booleans: True (1) \n",
    "# or False (0) for the question \"Is this athlete male?\"\n",
    "is_male = data[3,:].astype(bool)\n",
    "# The data on falls is formatted in the same way: 1 for a \n",
    "# fall/stumble, and 0 for a regular race. We can cast this\n",
    "# into Booleans too: True (1) of False (0) for \"Did this\n",
    "# athlete fall?\"\n",
    "fall = data[4,:].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a feel for the data\n",
    "\n",
    "Let's plot the data! Make sure to plot the (near) falls in a different colour, so we can see whether they really are different from the normal races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ready-start interval on the x-axis, and the 500\n",
    "# meter times on the y-axis.\n",
    "pyplot.plot(interval[fall==False], time_500m[fall==False], \\\n",
    "    'o', color=\"#FF69B4\")\n",
    "# Also plot the races in which athletes fell or stumbled.\n",
    "pyplot.plot(interval[fall==True], time_500m[fall==True], \\\n",
    "    'o', color=\"#006900\")\n",
    "\n",
    "# Add axis labels to make the plot clearer.\n",
    "pyplot.xlabel(\"Ready-start interval (sec)\", fontsize=18)\n",
    "pyplot.ylabel(\"Finish time (sec)\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first thing you see is that there are two REALLY slow races, both due to falls. Let's set a limit on the y-axis that will allow us to actually see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ready-start interval on the x-axis, and the\n",
    "# 500 meter times on the y-axis.\n",
    "pyplot.plot(interval[fall==False], time_500m[fall==False], \\\n",
    "    'o', color=\"#FF69B4\")\n",
    "# Also plot the races in which athletes fell or stumbled.\n",
    "pyplot.plot(interval[fall==True], time_500m[fall==True], \\\n",
    "    'o', color=\"#006900\")\n",
    "\n",
    "# Add axis labels to make the plot clearer.\n",
    "pyplot.xlabel(\"Ready-start interval (sec)\", fontsize=18)\n",
    "pyplot.ylabel(\"Finish time (sec)\", fontsize=18)\n",
    "\n",
    "# Set the axis limit.\n",
    "pyplot.ylim([34, 41])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear there are two separate sub-groups in the data here... Let's see whether that corresponds with sex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALE\n",
    "# Plot the ready-start interval on the x-axis, and the 500\n",
    "# meter times on the y-axis.\n",
    "pyplot.plot(interval[(is_male == True) & (fall==False)], \\\n",
    "    time_500m[(is_male == True) & (fall==False)], 'o', \\\n",
    "    color=\"#4e9a06\", label=\"Men\")\n",
    "# Also plot the races in which athletes fell or stumbled.\n",
    "pyplot.plot(interval[(is_male == True) & (fall==True)], \\\n",
    "    time_500m[(is_male == True) & (fall==True)], 'o', \\\n",
    "    color=\"#8ae234\")\n",
    "\n",
    "# FEMALE\n",
    "# Plot the ready-start interval on the x-axis, and the 500 \n",
    "# meter times on the y-axis.\n",
    "pyplot.plot(interval[(is_male == False) & (fall==False)], \\\n",
    "    time_500m[(is_male == False) & (fall==False)], 'o', \\\n",
    "    color=\"#c4a000\", label=\"Women\")\n",
    "# Also plot the races in which athletes fell or stumbled.\n",
    "pyplot.plot(interval[(is_male == False) & (fall==True)], \\\n",
    "    time_500m[(is_male == False) & (fall==True)], 'o', \\\n",
    "    color=\"#fce94f\")\n",
    "\n",
    "# Add axis labels to make the plot clearer.\n",
    "pyplot.xlabel(\"Ready-start interval (sec)\", fontsize=18)\n",
    "pyplot.ylabel(\"Finish time (sec)\", fontsize=18)\n",
    "\n",
    "# Set the axis limit.\n",
    "pyplot.ylim([34, 41])\n",
    "\n",
    "# Add a legend.\n",
    "pyplot.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that on average, men were about 4 seconds quicker than women. That means we can't just compute a Pearson correlation on the entire dataset as one group: Clearly there are two separate underlying distributions. However, we don't really care about the difference between men and women here. Instead, we're only interested in whether or not there is an effect of ready-start interval on finish times.\n",
    "\n",
    "In order to look at this, we could z-score the data within each group. This subtracts the group mean from every sample, and then divides it by the group standard deviation. The z-scored finish times for men and women should both have a mean of 0 and a standard deviation of 1, making the two groups directly comparable. (And, more importantly, combinable!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Compute the z-scored finish times.\n",
    "z_time_500m_male = \\\n",
    "    zscore(time_500m[(is_male==True) & (fall==False)])\n",
    "z_time_500m_female = \\\n",
    "    zscore(time_500m[(is_male==False) & (fall==False)])\n",
    "\n",
    "# Combine the two vectors into one.\n",
    "z_time_500m = \\\n",
    "    numpy.hstack([z_time_500m_male, z_time_500m_female])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pearson correlation function.\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute the correlation between interval and finish time.\n",
    "r, p = pearsonr(interval[fall==False], z_time_500m)\n",
    "\n",
    "pyplot.plot(interval[fall==False], z_time_500m, 'o')\n",
    "\n",
    "\n",
    "\n",
    "print(\"R={}, p={}\".format(round(r, ndigits=2), \\\n",
    "    round(p, ndigits=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we now know that there is a *statistically* significant correlation between ready-start interval and finish time. However, we don't know whether this correlation is *practically* significant. Ideally, we would like to know exactly how a longer ready-start interval affects the finish time. In other words: If the referee waits 1 second longer to shoot the starting pistol, how much slower does an athlete become?\n",
    "\n",
    "To answer this question, we can use *regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "One thing that wasn't mentioned about the 500 meter sprint in speed skating, is that athletes race twice. After the first round of races, pairs are mixed up, and all athletes race again. Their times of both races are combined, and whoever has the lowest summed time wins a gold medal.\n",
    "\n",
    "This is not unlike an experimental manipulation: It sounds a bit like a researcher used two trials per participant to estimate the effect of ready-start interval on finish time. This is exactly how you can use the data!\n",
    "\n",
    "For our each athlete, you can compute the difference in ready-start interval between both races. This will be your *predictor* variable `x`.\n",
    "\n",
    "You can also compute the difference in finish times between both races. This will be your *outcome* variable `y`.\n",
    "\n",
    "In a regression, you try to predict the outcome with one or more predictors. Or, in an equation:\n",
    "\n",
    "$y = \\beta_{0} + x_{1} \\beta_{1} + \\epsilon$\n",
    "\n",
    "Where $y$ is the outcome variable, $\\beta_{0}$ is the intercept (what is $y$ when all $x$ values are zero?),  $x_{1}$ is the first predictor variable, $\\beta_{1}$ is a free variable that determines how much $x_{1}$ affects $y$, and $\\epsilon$ is the *error term* that determines how much was unaccounted for. Sometimes this is called *noise*, because it refers to all unpredicted things.\n",
    "\n",
    "Or, in code:\n",
    "\n",
    "```python\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute our predictor and outcome: The differences between the two races of each athlete in ready-start interval and finish time. We can load the data for individual races from the file `speed_skating_paired_races.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Load the data from all individual races.\n",
    "data = numpy.loadtxt(\"speed_skating_paired_races.csv\", \\\n",
    "    delimiter=\",\", dtype=float, skiprows=1, \\\n",
    "    usecols=range(1,9), unpack=True)\n",
    "\n",
    "# Rename the variables for our convenience.\n",
    "interval_1 = data[0,:]\n",
    "time_100m_1 = data[1,:]\n",
    "time_500m_1 = data[2,:]\n",
    "interval_2 = data[3,:]\n",
    "time_100m_2 = data[4,:]\n",
    "time_500m_2 = data[5,:]\n",
    "is_male = data[6,:].astype(bool)\n",
    "exclude = data[7,:].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the differences in ready-start interval and finish time between the two races. We'll exclude all the athletes who (nearly) fell, but we won't separate men and women again. This is because we have no reason to assume that alerting effects are any different between men and women, and thus our hypothesis should be that *all* athletes are affected, regardless of sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the interval difference between race 1 and 2.\n",
    "interval_d = \\\n",
    "    interval_1[exclude==False] - interval_2[exclude==False]\n",
    "# Compute the finish time difference between race 1 and 2.\n",
    "time_500m_d = \\\n",
    "    time_500m_1[exclude==False] - time_500m_2[exclude==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check to see what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the values.\n",
    "pyplot.plot(interval_d, time_500m_d, 'o', color=\"#FF69B4\")\n",
    "# Add axis labels.\n",
    "pyplot.xlabel(\"Ready-start interval difference (sec)\", fontsize=18)\n",
    "pyplot.ylabel(\"Finish time difference (sec)\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph, we can see that ready-start interval differences lie between -1 and 1 seconds. We can also see that finish times are quite stable within each individual athlete: Most differences are between -0.4 and 0.4 seconds!\n",
    "\n",
    "In addition, just by eyeballing the graph, it looks like there might be a positive correlation between the ready-start interval difference and the finish time difference. Let's quantify this relation by using a regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "slope, intercept, r, p, std_err = \\\n",
    "    linregress(interval_d, time_500m_d)\n",
    "\n",
    "print(\"R={}, p={}\".format(round(r, ndigits=2), \\\n",
    "    round(p, ndigits=3)))\n",
    "print(\"slope=%.2f, intercept=%.2f\" % (slope, intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can learn that there is a statistically significant correlation with a Pearson R of 0.35. In addition, we now know how to quantify the effect:\n",
    "\n",
    "$y = 0.03 + 0.17 x_{1}$\n",
    "\n",
    "or:\n",
    "\n",
    "$\\Delta_{finish} = 0.03 + 0.17 \\Delta_{interval}$\n",
    "\n",
    "In practice, this means that for every second the referee waits between \"Ready\" and the starting shot, they add (on average) 0.17 seconds to an atheletes finish time. You can draw this line into your graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a bunch of x values that will cover the range of\n",
    "# the interval difference data.\n",
    "x = numpy.arange(-1, 1, 0.1)\n",
    "# Compute what the predicted y values would be for each\n",
    "# of these values.\n",
    "y = intercept + slope * x\n",
    "\n",
    "# Plot the line into a graph.\n",
    "pyplot.plot(x, y, '-', linewidth=3, color=\"#FF69B4\")\n",
    "\n",
    "# Plot the measured values.\n",
    "pyplot.plot(interval_d, time_500m_d, 'o', color=\"#FF69B4\", \\\n",
    "    alpha=0.5)\n",
    "\n",
    "# Add axis labels.\n",
    "pyplot.xlabel(\"Ready-start interval difference (sec)\", \\\n",
    "    fontsize=18)\n",
    "pyplot.ylabel(\"Finish time difference (sec)\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a problem? Well, at Vancouver in 2010, the total difference between gold and silver was 0.16 seconds. It is not uncommen at all for speed skaters to get even closer: In 2014, the difference between gold and silver was 0.01 seconds, and between gold and bronze it was 0.15. In 2018, the difference between gold and silver was 0.01 seconds again.\n",
    "\n",
    "Clearly, the margins are small, and thus this ready-start interval effect might have real-life consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "If you're interested in the background to the data, you can read the following two articles. They're very short, and not very technical:\n",
    "\n",
    "- Dalmaijer, E.S., Nijenhuis, B.G., & Van der Stigchel, S. (2015). Life is unfair, and so are racing sports: Some athletes can randomly benefit from alerting effects due to inconsistent starting procedures. Frontiers in Psychology, 6(1618). doi:[10.3389/fpsyg.2015.01618](http://dx.doi.org/10.3389/fpsyg.2015.01618)\n",
    "- Dalmaijer, E.S., Nijenhuis, B.G., & Van der Stigchel, S. (2016). Commentary: Life is unfair, and so are racing sports: Some athletes can randomly benefit from alerting effects due to inconsistent starting procedures. Frontiers in Psychology, 7(119). doi:[10.3389/fpsyg.2016.00119](http://dx.doi.org/10.3389/fpsyg.2016.00119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But how does regression work?\n",
    "\n",
    "In regression, you have an explicit *model* for your data. Specifically, it says that there is a linear relationship between variables $y$ and $x$:\n",
    "\n",
    "$y = \\beta_{0} + x_{1} \\beta_{1} + \\epsilon$\n",
    "\n",
    "You know what the values for $y$ are, because you measured those. You also know what the values for $x$ are, because you manipulated (or measured) those. But how do you know what the $\\beta$ values are? And what that $\\epsilon$ is?\n",
    "\n",
    "One way would be to simply try all possible values of each $\\beta$, and see when the resulting line fits best. How do you know what set of $\\beta$ values fits best? Simple: It's when the difference between your predicted values of $y$ and your measured values of $y$ is the smallest.\n",
    "\n",
    "Let's give this approach a go with the skating dataset. (As you've hopefully been doing with all snippets; read the comments for explanations of specific lines!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the ranges along which we need to \n",
    "# search for the best fitting betas.\n",
    "b0_range = numpy.arange(0, 1, 0.01)\n",
    "b1_range = numpy.arange(-10, 10, 0.01)\n",
    "\n",
    "# Count the number of values we will try for each beta.\n",
    "n_b0 = len(b0_range)\n",
    "n_b1 = len(b1_range)\n",
    "\n",
    "# Second, define some starting values. The first are the\n",
    "# betas, which will be None to start with.\n",
    "beta = (None, None)\n",
    "# We also need to start with a difference between the y \n",
    "# values and the predicted y values. This\n",
    "# will start at infinitely high:\n",
    "min_s = numpy.inf\n",
    "\n",
    "# Finally, we loop through every possible combination of \n",
    "# b0 and b1.\n",
    "for i, b0 in enumerate(b0_range):\n",
    "    for j, b1 in enumerate(b1_range):\n",
    "        # Predict y using the current betas.\n",
    "        y_pred = b0 + b1 * interval_d\n",
    "        # Compute the difference between the predicted y \n",
    "        # and the measured y for each observation.\n",
    "        d = time_500m_d - y_pred\n",
    "        # Compute the sum of squares of the differences \n",
    "        # (residuals).\n",
    "        s = numpy.sum(d**2)\n",
    "        # Remember the current betas if the sum of squares\n",
    "        # is lower than the previously lowest.\n",
    "        if s < min_s:\n",
    "            betas = (b0, b1)\n",
    "            min_s = s\n",
    "\n",
    "print(\"Best fit: b0={}, b1={}\".format(round(betas[0], ndigits=2), \\\n",
    "    round(betas[1], ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might recall, these are the same values that you obtained through using the `linregress` function earlier!\n",
    "\n",
    "The reason this works, is because you travelled through *parameter space*, and at each point computed the *sum of squares* of the difference between the actual and your predicted values. This difference is called the *residuals*. You kept track of which point in parameter space was associated with the lowest *squared residuals*. This is called *least-squares regression*.\n",
    "\n",
    "You can actually plot parameter space and the associated residual squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the ranges along which we need to search \n",
    "# for the best fitting betas.\n",
    "b0_range = numpy.arange(0, 10, 0.01)\n",
    "b1_range = numpy.arange(-10, 10, 0.01)\n",
    "\n",
    "# Count the number of values we will try for each beta.\n",
    "n_b0 = len(b0_range)\n",
    "n_b1 = len(b1_range)\n",
    "\n",
    "# Second, define some starting values. The first are the\n",
    "# betas, which will be None to start with.\n",
    "beta = (None, None)\n",
    "# We also need to start with a difference between the y\n",
    "# values and the predicted y values. This\n",
    "# will start at infinitely high:\n",
    "min_s = numpy.inf\n",
    "# Keep track of the residuals at every point in parameter\n",
    "# space. This starts as a matrix filled with NaN (not a\n",
    "# number), and one value will be added on every iteration.\n",
    "s = numpy.zeros((n_b0,n_b1), dtype=float) * numpy.NaN\n",
    "\n",
    "# Finally, we loop through every possible combination of \n",
    "# b0 and b1.\n",
    "for i, b0 in enumerate(b0_range):\n",
    "    for j, b1 in enumerate(b1_range):\n",
    "        # Predict y using the current betas.\n",
    "        y_pred = b0 + b1 * interval_d\n",
    "        # Compute the difference between the predicted y \n",
    "        # and the measured y for each observation.\n",
    "        d = time_500m_d - y_pred\n",
    "        # Compute the sum of squares of the differences\n",
    "        # (residuals).\n",
    "        s[i,j] = numpy.sum(d**2)\n",
    "        # Remember the current betas if the sum of squares\n",
    "        # is lower than the previously lowest.\n",
    "        if s[i,j] < min_s:\n",
    "            betas = (b0, b1)\n",
    "            min_s = s[i,j]\n",
    "\n",
    "print(\"Best fit: b0={}, b1={}\".format(round(betas[0], ndigits=2), \\\n",
    "    round(betas[1], ndigits=2)))\n",
    "\n",
    "# Now plot the residual squares in parameter space:\n",
    "pyplot.imshow(s, cmap=\"hot\")\n",
    "# Set the tick labels on the x and y axes.\n",
    "x_ticks = [0, n_b1//2, n_b1-1]\n",
    "x_tick_labels = numpy.round(b1_range[x_ticks])\n",
    "pyplot.xticks(x_ticks, x_tick_labels)\n",
    "pyplot.xlabel(\"Possible beta 1 values\")\n",
    "y_ticks = [0, n_b0-1]\n",
    "y_tick_labels = numpy.round(b0_range[y_ticks])\n",
    "pyplot.yticks(y_ticks, y_tick_labels)\n",
    "pyplot.ylabel(\"Possible beta 0 values\")\n",
    "\n",
    "# Draw an colour bar to show the resulting sums of residual\n",
    "# squares.\n",
    "pyplot.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, you can see that the optimal combination of betas is close to point (0,0). However, you can also see that we cast a *very* wide net. Perhaps it would have been better to choose a smaller search space. For example, instead of using ranges $[0,10]$ for $\\beta_{0}$ and $[-10,10]$ for $\\beta_{1}$, we could have used $[0,0.1]$ and $[0,0.5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the ranges along which we need to \n",
    "# search for the best fitting betas.\n",
    "b0_range = numpy.arange(0, 0.1, 0.001)\n",
    "b1_range = numpy.arange(0, 0.5, 0.001)\n",
    "\n",
    "# Count the number of values we will try for each beta.\n",
    "n_b0 = len(b0_range)\n",
    "n_b1 = len(b1_range)\n",
    "\n",
    "# Second, define some starting values. The first are the\n",
    "# betas, which will be None to start with.\n",
    "beta = (None, None)\n",
    "# We also need to start with a difference between the y\n",
    "# values and the predicted y values. This\n",
    "# will start at infinitely high:\n",
    "min_s = numpy.inf\n",
    "# Keep track of the residuals at every point in parameter\n",
    "# space. This starts as a matrix filled with NaN (not a \n",
    "# number), and one value will be added on every iteration.\n",
    "s = numpy.zeros((n_b0,n_b1), dtype=float) * numpy.NaN\n",
    "\n",
    "# Finally, we loop through every possible combination of \n",
    "# b0 and b1.\n",
    "for i, b0 in enumerate(b0_range):\n",
    "    for j, b1 in enumerate(b1_range):\n",
    "        # Predict y using the current betas.\n",
    "        y_pred = b0 + b1 * interval_d\n",
    "        # Compute the difference between the predicted y \n",
    "        # and the measured y for each observation.\n",
    "        d = time_500m_d - y_pred\n",
    "        # Compute the sum of squares of the differences \n",
    "        # (residuals).\n",
    "        s[i,j] = numpy.sum(d**2)\n",
    "        # Remember the current betas if the sum of squares\n",
    "        # is lower than the previously lowest.\n",
    "        if s[i,j] < min_s:\n",
    "            betas = (b0, b1)\n",
    "            min_s = s[i,j]\n",
    "\n",
    "print(\"Best fit: b0={}, b1={}\".format(round(betas[0], ndigits=2), \\\n",
    "    round(betas[1], ndigits=2)))\n",
    "\n",
    "# Now plot the residual squares in parameter space:\n",
    "pyplot.imshow(s, cmap=\"hot\")\n",
    "# Set the tick labels on the x and y axes.\n",
    "x_ticks = [0, n_b1//2, n_b1-1]\n",
    "x_tick_labels = numpy.round(b1_range[x_ticks], 2)\n",
    "pyplot.xticks(x_ticks, x_tick_labels)\n",
    "pyplot.xlabel(\"Possible beta 1 values\")\n",
    "y_ticks = [0, n_b0-1]\n",
    "y_tick_labels = numpy.round(b0_range[y_ticks], 2)\n",
    "pyplot.yticks(y_ticks, y_tick_labels)\n",
    "pyplot.ylabel(\"Possible beta 0 values\")\n",
    "\n",
    "# Draw an colour bar to show the resulting sums of residual\n",
    "# squares.\n",
    "pyplot.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it's a lot clearer that the best fitting combination of predictor values is around (0.17, 0.03)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing your own least-squares regression\n",
    "\n",
    "What you just did is a *full space estimate*. The advantage of such an approach is that you tried every possible point within a pre-defined grid. A significant downside, however, is that it takes ages to complete. Especially if you have no clue where your possible $\\beta$ values are going to be, and/or if you want a high resolution estimate (smaller step sizes between your points), you would have to try a very large number of combinations. In addition, if you want to add additional predictors, your grid will grow exponentially.\n",
    "\n",
    "Fortunately, there are *minimisation algorithms*. These will walk through parameter space in a clever way. Most work by randomly starting at one particular point, computing the residual squares for that particular set of $\\beta$ values, and then they try a nearby point to compute the residual squares again. By using the slope between these points, the algorithm knows where to go: Because the best fitting solution is at the point with the lowest residual square, the algorithm simply has to follow the slope downwards until it reaches a point where it can no longer go down any further. This is the best fit!\n",
    "\n",
    "Let's try one of these minimisation algorithms. It needs a function to minimise the value for (i.e. a function that computes the residual squares):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the minimize function from SciPy.\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define a function to compute the sum of residual \n",
    "# squares based on our model.\n",
    "def residuals(betas, x, y):\n",
    "    # Compute the predicted value of y.\n",
    "    y_pred = betas[0] + betas[1] * x\n",
    "    # Compute the residuals.\n",
    "    res = y - y_pred\n",
    "    # Compute the sum of squared residuals.\n",
    "    s = numpy.sum(res**2)\n",
    "    # Return the squared residuals.\n",
    "    return s\n",
    "\n",
    "# Choose values that the algorithm uses as an initial guess.\n",
    "initial_guess = (0.0, 0.0)\n",
    "# Use the mimize function to compute the best fit.\n",
    "model = minimize(residuals, initial_guess, \\\n",
    "    args=(interval_d, time_500m_d), method=\"L-BFGS-B\")\n",
    "\n",
    "# Report the betas.\n",
    "betas = model.x\n",
    "print(\"Best fit: b0={}, b1={}\".format(round(betas[0], ndigits=2), \\\n",
    "    round(betas[1], ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same result as the full space estimate provided earlier. However, this method is **much** faster.\n",
    "\n",
    "You could ask what the benefit of using `minimize` is over simply using `linregress`. Both functions allowed us to fit our data, both gave us the same answer, and both were very quick about it too. The neat thing about using the `minimize` approach is that it is very flexible. You could have used *any* model, regardless of how many predictors you would have liked. You could even have used a non-linear model. Or a different way of computing the \"best fit\", for example one that doesn't rely on residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is a fit?\n",
    "\n",
    "The one thing we didn't do yet, is computing how good a fit really is. The usual measure for this is the *coefficient of determination*, or $R^{2}$. This is computed by dividing the sum of squares of the residuals by the total sum of squares:\n",
    "\n",
    "$R^{2} = 1 - {{SS_{res}} \\over {SS_{total}}}$\n",
    "\n",
    "Or, more scary-looking, but also more helpful:\n",
    "\n",
    "$R^{2} = 1 - { { \\Sigma^{n}_{i=1} (y_{i} - f_{i})^{2} }  \\over { \\Sigma^{n}_{i=1}(y_{i} - \\bar{y})^{2} }}$\n",
    "\n",
    "Where $n$ is the number of observations, $\\bar{y}$ is the mean of $y$, and $f_{i}$ is the predicted value of $y_{i}$ given the model. For example:\n",
    "\n",
    "$f_{i} = \\beta_{0} + \\beta_{1} x_{i}$\n",
    "\n",
    "Or, in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted y values based on the fitted betas.\n",
    "y_pred = betas[0] + betas[1] * interval_d\n",
    "\n",
    "# Compute the residual sum of squares.\n",
    "ss_res = numpy.sum((time_500m_d - y_pred)**2)\n",
    "\n",
    "# Compute the total sum of squares.\n",
    "ss_tot = numpy.sum((time_500m_d - numpy.mean(time_500m_d))**2)\n",
    "\n",
    "# Compute R square.\n",
    "r_sq = 1.0 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"R squared = {}\".format(round(r_sq, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $R^{2}$ is 1, all variance in outcome $y$ is predicted by predictor $x$. If $R^{2}$ is 0, none of the variance in outcome $y$ is explained by predictor $x$. Here, the ready-start interval differences can explain 12% of the variance in finish time difference.\n",
    "\n",
    "In the case of high-level sporting events, the amount of variance explained by anything other than athlete's ability should ideally be 0%. Here, a random variation in pre-start time that is introduced by the person holding the starting pistol was 12%. That's probably not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable regression\n",
    "\n",
    "Now that you know how to do a regression with a single predictor, we can turn to regressions with multiple predictors. The general format of multi-variable regression looks very similar to the single-variable version:\n",
    "\n",
    "$y = \\beta_{0} + x_{1} \\beta_{1} + ... + x_{n} \\beta_{n} + \\epsilon$\n",
    "\n",
    "Where $n$ is the number of variables you might have. For example, the equation for three predictors would look like this:\n",
    "\n",
    "$y = \\beta_{0} + x_{1} \\beta_{1} + x_{2} \\beta_{2} + x_{3} \\beta_{3} + \\epsilon$\n",
    "\n",
    "Here, $x_{1}$, $x_{2}$, and $x_{3}$ are three different predictors. $\\beta_{1}$, $\\beta_{2}$, and $\\beta_{3}$ indicate the magnitude and direction of the effect of each predictor on $y$. As before, $\\epsilon$ captures the \"noise\": all variance in $y$ that we could not explain using the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example dataset\n",
    "\n",
    "Last session, we looked at a dataset that contained the number of minutes each participant listened to Taylor Swift, and their happiness ratings. We collected similar data, but now included a measure of IQ too. These data can be found in the attached file `taytay_revisited.csv`.\n",
    "\n",
    "We can load the dataset using NumPy's `loadtxt` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Load the data.\n",
    "data = numpy.loadtxt(\"taytay_revisited.csv\", dtype=float, \\\n",
    "    delimiter=\",\", skiprows=1, unpack=True)\n",
    "\n",
    "# Create some easy variable names to point to the data.\n",
    "tay_minutes = data[0,:]\n",
    "happy = data[1,:]\n",
    "iq = data[2,:]\n",
    "\n",
    "# Use the two predictors together into a single variable.\n",
    "predictors = numpy.vstack([iq, happy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to predict the number of minutes someone listens to Taylor Swift by using their IQ score and happiness rating. Or, in an equation:\n",
    "\n",
    "$Swifting = \\beta_{0} + happiness * \\beta_{1} + IQ * \\beta_{2} + \\epsilon$\n",
    "\n",
    "Let's write a function to model this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(betas, x, y):\n",
    "    # Compute the predicted y values.\n",
    "    y_pred = betas[0] + betas[1] * x[0,:] + \\\n",
    "        betas[2] * x[1,:]\n",
    "    # Compute the residuals.\n",
    "    res = y - y_pred\n",
    "    # Compute the sum of squared residuals.\n",
    "    s = numpy.sum(res**2)\n",
    "    # Return the SSres\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use SciPy's `minimize` function to fit out model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set an initial guess for the betas.\n",
    "initial_guess = [0.0, 0.0, 0.0]\n",
    "\n",
    "# Fit the model.\n",
    "model = minimize(residuals, initial_guess, \\\n",
    "    args=(predictors, tay_minutes), method=\"L-BFGS-B\")\n",
    "\n",
    "# Report the betas.\n",
    "betas = model.x\n",
    "print(\"Best fit: b0={}, b1={}, b2={}\".format( \\\n",
    "    round(betas[0], ndigits=2), round(betas[1], ndigits=2), \\\n",
    "    round(betas[2], ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So the best fit is the following:\n",
    "\n",
    "$Swifting = 34.19 + happiness * 1.88 + IQ * 0.16 + \\epsilon$\n",
    "\n",
    "Our $\\beta$ values are 1.88 for happiness and 0.16 for IQ. Does that mean IQ is less important than happiness for determining Taylor Swift listening? It doesn't necessarily, because we're currently looking at *unstandardised coefficients*. The values for IQ are larger than the values for happiness: IQ, per definition, has a mean of 100 and a standard deviation of 15, whereas happiness was rated on a 0-10 scale. This difference in range alters the magnitudes of $\\beta$ values.\n",
    "\n",
    "In order to directly compare the parameters, we'll need the *standardised coefficients*. You can compute those by simply z-scoring predictors (and outcomes!) **before** running your regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Use the two predictors together into a single variable.\n",
    "z_predictors = numpy.vstack([zscore(iq), zscore(happy)])\n",
    "z_minutes = zscore(tay_minutes)\n",
    "\n",
    "# Set an initial guess for the betas.\n",
    "initial_guess = [1.0, 1.0, 1.0]\n",
    "\n",
    "# Fit the model.\n",
    "stand_model = minimize(residuals, initial_guess, \\\n",
    "    args=(z_predictors, z_minutes), method=\"L-BFGS-B\")\n",
    "\n",
    "# Report the betas.\n",
    "stand_betas = stand_model.x\n",
    "print(\"Best fit: b0={}, b1={}, b2={}\".format( \\\n",
    "    round(stand_betas[0], ndigits=2), \\\n",
    "    round(stand_betas[1], ndigits=2), \\\n",
    "    round(stand_betas[2], ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can really tell that happiness has a bigger effect on Taylor Swift listening than IQ does.\n",
    "\n",
    "Let's compute how much of the variance we can explain with the current fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted y values based on the fitted betas.\n",
    "y_pred = betas[0] + betas[1] * iq + betas[2] * happy\n",
    "\n",
    "# Compute the residual sum of squares.\n",
    "ss_res = numpy.sum((tay_minutes - y_pred)**2)\n",
    "\n",
    "# Compute the total sum of squares.\n",
    "ss_tot = numpy.sum((tay_minutes - numpy.mean(tay_minutes))**2)\n",
    "\n",
    "# Compute R square.\n",
    "r_sq = 1.0 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"R squared = {}\".format(round(r_sq, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $R^{2}$ of 0.11 is pretty decent!\n",
    "\n",
    "Or is it? We don't really know what it means in context, or whether it's statistically significant. Let's shelf that thought for a second, as we will return to it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting\n",
    "\n",
    "So we just learned about linear regression and least-squares estimation. We can extend this topic is extended into more general *model fitting*. We will be thinking about scientific hypotheses as *models*, which are precise descriptions of relationships between variables. For example, according to Newton's second law of motion, the relationship between an object's force, mass, and acceleration is given by the equation $F = ma$\n",
    "\n",
    "Models do not have to be linear, but can take all shapes and sizes. The examples we'll look at today will include an exponential function, and a probability density function that is a mixture of typical distributions.\n",
    "\n",
    "In Experimental Psychology, entirely descriptive box-and-arrow drawings are often mistaken for models. Although they might be useful for describing phenomena, it can be argued that they lack predictive power. Here, we'll go by a more narrow definition, and only use the word *model* for hypotheses that are testable and precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a non-linear model with one free parameter\n",
    "\n",
    "The first example today is in the field of short-term memory. Storing visual information in short-term memory requires that stimuli in the environment are processed into an internal representation, and that this representation is maintained temporarily (typically for up to a few seconds). In the past, researchers have wondered whether human's faced storage capacity limits for visual information. In addition, they wondered whether there are limitations on the processing capacity. In other words, researchers wanted to know whether there was a maximum limit on the amount of information a person can have in short-term memory, and whether there is a maximum bandwith available for encoding information into short-term memory.\n",
    "\n",
    "One experiment used in the 1980s and 1990s is the *whole report* paradigm. In this type of experiment, participants are presented with a number of visual stimuli. For example, they could be presented with 8 different letters. The researcher would control how long the letters were visible before they were masked (replaced by a neutral stimulus). In this way, the researcher could control exactly how long participants could process the presented stimuli.\n",
    "\n",
    "What is measured in whole report experiments is how many of the stimuli participants could remember. At lower exposure durations, one would expect fewer stimuli to be processed, especially if humans have a limited processing capacity. In addition, if human short-term memory is of a limited storage capacity, one expects that subjects would struggle to recall all stimuli even under very long exposure durations.\n",
    "\n",
    "More precisely, one would expect that the number of recalled stimuli rises as a function of the exposure duration (the slope determined by processing capacity), but that this rise is limited by an asymptote (storage capacity).\n",
    "\n",
    "Let's define the things we just said a bit more precisely. If the processing capacity (let's call it $C$) is limited, it should be divided among the number of stimuli that a participant is trying to recall (let's call that number $T$). So our capacity per stimulus is:\n",
    "\n",
    "${{C} \\over {T}}$\n",
    "\n",
    "To find how much of a stimulus was encoded, we need to multiply the processing capacity by the time that a participant was processing a stimulus (let's call that $\\tau$):\n",
    "\n",
    "${{C \\tau} \\over {T}}$\n",
    "\n",
    "This can be used to describe the *probability of a stimulus finishing processing* (let's call that $s$). The following equation does just that:\n",
    "\n",
    "$s = 1 - \\exp{({{-C \\tau} \\over {T}})}$\n",
    "\n",
    "Note that, if there was no limit on processing capacity, we wouldn't have to divide the total capacity by the number of stimuli. Which would look like this:\n",
    "\n",
    "$s = 1 - \\exp{(-C \\tau)}$\n",
    "\n",
    "For now, let's assume there were fewer stimuli than participants could remember. In that case, the storage capacity wouldn't matter. The number of recalled stimuli (let's call it $E(score)$) could thus be computed by simply multiplying the probability that any one stimulus finishes encoding ($s$) by the number of stimuli ($T$):\n",
    "\n",
    "$E(score) = Ts$\n",
    "\n",
    "OK, let's recap: We have two models. In the first model, the limited processing capacity has to be divided over all stimuli. In the second model, the processing capacity does not have to be divided at all. The equations for these two models look like this:\n",
    "\n",
    "$E(score)_{1} = T (1 - \\exp{({{-C \\tau} \\over {T}})})$\n",
    "\n",
    "$E(score)_{2} = T (1 - \\exp{(-C \\tau)})$\n",
    "\n",
    "Note that we know $T$, as this is the number of stimuli in our experiment. We also know $\\tau$, as this is the time participants were given to encode the stimuli.\n",
    "\n",
    "In an experiment, participants were always shown 3 stimuli. These stimuli were letters that participants were asked to type in after a brief delay. In each trial, stimuli were presented with a different exposure duration (time between the onset of the stimuli and the onset of the mask). These exposure durations could be 10, 20, 40, 80, 160, 320 milliseconds.\n",
    "\n",
    "The average number of stimuli that each participant could recall after each exposure duration has already been computed for you. You can find these data in the attached file `whole_report.csv`. Load it using NumPy's `loadtxt` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Load the data from the data file.\n",
    "data = numpy.loadtxt(\"whole_report.csv\", dtype=float, \\\n",
    "    delimiter=\",\", skiprows=1, unpack=True)\n",
    "\n",
    "# Create a variable that codes the number of\n",
    "# stimuli used in each trial of this experiment.\n",
    "n_stimuli = 3\n",
    "\n",
    "# Create a vector with all exposure durations from\n",
    "# the experiment.\n",
    "expdur = numpy.array([0.01,0.02,0.04,0.08,0.16,0.32], \\\n",
    "    dtype=float)\n",
    "\n",
    "# Convenience renaming of columns in the data file.\n",
    "# We won't actually use this, it's more to show you\n",
    "# what is in the file if you didn't open it outside\n",
    "# of this workbook.\n",
    "score_10ms =  data[0,:]\n",
    "score_20ms =  data[1,:]\n",
    "score_40ms =  data[2,:]\n",
    "score_80ms =  data[3,:]\n",
    "score_160ms = data[4,:]\n",
    "score_320ms = data[5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's visualise it to have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "# Count the number of participants.\n",
    "n_participants = data.shape[1]\n",
    "\n",
    "# Loop through all participants, and plot the data for each.\n",
    "for i in range(n_participants):\n",
    "    # Plot the exposure duration on the x-axis, and the\n",
    "    # average score for the current participant on the\n",
    "    # y-axis.\n",
    "    pyplot.plot(expdur, data[:,i], alpha=0.2)\n",
    "\n",
    "# Add axis labels to the plot.\n",
    "pyplot.xlabel(\"Exposure duration (seconds)\", fontsize=16)\n",
    "pyplot.ylabel(\"Average number of recalled stimuli\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having all participants in one plot is a bit messy. Let's compute the average, and plot that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average score across all participants.\n",
    "m = numpy.mean(data, axis=1)\n",
    "# Compute the standard deviation.\n",
    "sd = numpy.std(data, axis=1, ddof=1)\n",
    "# Compute the standard error of the mean.\n",
    "sem = sd / numpy.sqrt(n_participants)\n",
    "\n",
    "# Plot the average and the standard error of the mean.\n",
    "pyplot.plot(expdur, m, color=\"#FF69B4\")\n",
    "pyplot.fill_between(expdur, m-sem, m+sem, \\\n",
    "    color=\"#FF69B4\", alpha=0.3)\n",
    "\n",
    "# Add axis labels to the plot.\n",
    "pyplot.xlabel(\"Exposure duration (seconds)\", fontsize=16)\n",
    "pyplot.ylabel(\"Average number of recalled stimuli\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have had a look at the data, it's time to start fitting the models. You would typically do this for every participant, but for now let's practice on the mean instead.\n",
    "\n",
    "Fitting models to data works in much the same way as linear regression. First, you define a function to compute the residuals for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 was for a limited processing capacity.\n",
    "def model_1(parameters, T, x, y):\n",
    "    # The predicted y values based on model 1:\n",
    "    y_pred = T * (1 - numpy.exp((- parameters[0] * x) / T))\n",
    "    return y_pred\n",
    "\n",
    "# Model 2 was for an unlimited processing capacity.\n",
    "def model_2(parameters, T, x, y):\n",
    "    # The predicted y values based on model 2:\n",
    "    y_pred = T * (1 - numpy.exp(- parameters[0] * x))\n",
    "    return y_pred\n",
    "\n",
    "def residuals(parameters, T, x, y, model_nr):\n",
    "    # Compute the predicted y values based on a model.\n",
    "    if model_nr == 1:\n",
    "        y_pred = model_1(parameters, T, x, y)\n",
    "    elif model_nr == 2:\n",
    "        y_pred = model_2(parameters, T, x, y)\n",
    "    # Compute the difference between the measured outcome\n",
    "    # and the predicted outcome (the residuals).\n",
    "    res = y - y_pred\n",
    "    # Compute the sum of squared residuals.\n",
    "    s = numpy.sum(res**2)\n",
    "    # Return the squared residuals.\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've defined the models, you can find the best fitting parameters using a minimisation algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Go through all models.\n",
    "for i, model in enumerate([model_1, model_2]):\n",
    "    # Fit the current model.\n",
    "    outcome = minimize(residuals, [0], \\\n",
    "        args=(n_stimuli, expdur, m, i+1), \\\n",
    "        method=\"L-BFGS-B\", bounds=[(0,None)])\n",
    "    # Get the best parameters for the best fit.\n",
    "    C = outcome.x[0]\n",
    "    # Compute the residual sum of squares.\n",
    "    ss_res = residuals(outcome.x, n_stimuli, \\\n",
    "        expdur, m, i+1)\n",
    "\n",
    "    print(\"Model {}: C={}, SSres={}\".format( \\\n",
    "        i+1, round(C, ndigits=2), round(ss_res, ndigits=2)))\n",
    "\n",
    "    # Compute the predicted outcome values.\n",
    "    y_pred = model(outcome.x, n_stimuli, expdur, m)\n",
    "    # Plot the predicted outcome.\n",
    "    pyplot.plot(expdur, y_pred, '--', label=\"model %d\" % (i+1))\n",
    "\n",
    "# Plot the average.\n",
    "pyplot.plot(expdur, m, 'o', color=\"#000000\")\n",
    "# Add axis labels to the plot.\n",
    "pyplot.xlabel(\"Exposure duration (seconds)\", fontsize=16)\n",
    "pyplot.ylabel(\"Average number of recalled stimuli\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, which model would you say fits best?\n",
    "\n",
    "Why, from a mathematical perspective, do you think the two models fit equally well? (Think about the relationship between parameters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model with two free parameters\n",
    "\n",
    "Did you notice how at exposure durations of 10 milliseconds, participants tend not to recall any stimuli? Perhaps our assumption that participants can use the full exposure duration to process information does not hold true. Maybe participants need a bit of initial time to visually process the stimuli before they can start encoding them into short-term memory?\n",
    "\n",
    "This is an interesting suggestion, and one that can be written down as an equation. Recall our first model? It assumed that the processing time $\\tau$ was the entire exposure duration (let's call that $t$).\n",
    "\n",
    "$E(score)_{1} = T (1 - \\exp{({{-C \\tau} \\over {T}})})$\n",
    "\n",
    "*where $\\tau = t$*\n",
    "\n",
    "But perhaps there is a *minimally effective exposure duration*, i.e. a brief period during which no encoding into short-term memory occurs yet. Let's call that $t_{0}$. This value needs to be subtracted from the total exposure duration $t$:\n",
    "\n",
    "$\\tau = t - t_{0}$\n",
    "\n",
    "This means that our first model could be written like this:\n",
    "\n",
    "$E(score)_{1} = T (1 - \\exp{({{-C t} \\over {T}})})$\n",
    "\n",
    "And a version that does incorporate the minimally effective exposure duration could be written like this:\n",
    "\n",
    "$E(score)_{3} = T (1 - \\exp{({{-C (t - t_{0})} \\over {T}})})$\n",
    "\n",
    "We can also incorporate this in the models that did not have a limited processing capacity:\n",
    "\n",
    "$E(score)_{2} = T (1 - \\exp{(-C t)})$\n",
    "\n",
    "$E(score)_{4} = T (1 - \\exp{(-C (t - t_{0}))})$\n",
    "\n",
    "We should create functions for these new models too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 was for a limited processing capacity.\n",
    "def model_3(parameters, T, x, y):\n",
    "    # The predicted y values based on model 3:\n",
    "    y_pred = T * (1 - numpy.exp((- parameters[0] * \\\n",
    "        (x-parameters[1])) / T))\n",
    "    return y_pred\n",
    "\n",
    "# Model 2 was for an unlimited processing capacity.\n",
    "def model_4(parameters, T, x, y):\n",
    "    # The predicted y values based on model 4:\n",
    "    y_pred = T * (1 - numpy.exp(- parameters[0] * \\\n",
    "        (x-parameters[1])))\n",
    "    return y_pred\n",
    "\n",
    "def residuals(parameters, T, x, y, model_nr):\n",
    "    # Compute the predicted y values based on a model.\n",
    "    if model_nr == 1:\n",
    "        y_pred = model_1(parameters, T, x, y)\n",
    "    elif model_nr == 2:\n",
    "        y_pred = model_2(parameters, T, x, y)\n",
    "    elif model_nr == 3:\n",
    "        y_pred = model_3(parameters, T, x, y)\n",
    "    elif model_nr == 4:\n",
    "        y_pred = model_4(parameters, T, x, y)\n",
    "    # Compute the difference between the measured outcome\n",
    "    # and the predicted outcome (the residuals).\n",
    "    res = y - y_pred\n",
    "    # Compute the sum of squared residuals.\n",
    "    s = numpy.sum(res**2)\n",
    "    # Return the squared residuals.\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit ALL the models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Go through all models.\n",
    "for i, model in enumerate([model_1, model_2, model_3, model_4]):\n",
    "    # Fit the current model.\n",
    "    outcome = minimize(residuals, [0, 0], \\\n",
    "        args=(n_stimuli, expdur, m, i+1), \\\n",
    "        method=\"L-BFGS-B\", bounds=[(0,None), (0,None)])\n",
    "    # Get the best parameters for the best fit.\n",
    "    C = outcome.x[0]\n",
    "    t0 = outcome.x[1]\n",
    "    # Compute the residual sum of squares.\n",
    "    ss_res = residuals(outcome.x, n_stimuli, \\\n",
    "        expdur, m, i+1)\n",
    "\n",
    "    print(\"Model {}: C={}, SSres={}\".format( \\\n",
    "        i+1, round(C, ndigits=2), round(ss_res, ndigits=2)))\n",
    "\n",
    "    # Compute the predicted outcome values.\n",
    "    y_pred = model(outcome.x, n_stimuli, expdur, m)\n",
    "    # Plot the predicted outcome.\n",
    "    pyplot.plot(expdur, y_pred, '--', label=\"model %d\" % (i+1))\n",
    "\n",
    "# Plot the average.\n",
    "pyplot.plot(expdur, m, 'o', color=\"#000000\")\n",
    "# Add axis labels to the plot.\n",
    "pyplot.xlabel(\"Exposure duration (seconds)\", fontsize=16)\n",
    "pyplot.ylabel(\"Average number of recalled stimuli\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of our models do you think fits best now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Mind you, this is still a very narrow model: It just describes behaviour in a single task (although it was derived within a wider computational framework). In addition, there is a way to incorporate a storage capacity for short-term memory (which we have ignored here). For more info, you can read the following paper:*\n",
    "\n",
    "- Budesen, C. (1990). A theory of visual attention. *Psychological Review*, *97*(4), 523-547. doi:[10.1037/0033-295X.97.4.523](http://dx.doi.org/10.1037/0033-295X.97.4.523)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "One way to compare the models introduced above, is by measuring how much variance each explains. The logic behind this is that a better model should explain more variance. You could compute the coefficient of determination for each model, using the same equation that you used in linear regression last week:\n",
    "\n",
    "$R^{2} = 1 - { { \\Sigma^{n}_{i=1} (y_{i} - f_{i})^{2} }  \\over { \\Sigma^{n}_{i=1}(y_{i} - \\bar{y})^{2} }}$\n",
    "\n",
    "Where $y$ is the measured outcome (correctly recalled number of stimuli) for each participant $i$, and $f$ is the predicted number of stimuli for each participant.\n",
    "\n",
    "Let's run the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all models.\n",
    "for i, model in enumerate([model_1, model_2, model_3, model_4]):\n",
    "    # Fit the current model.\n",
    "    outcome = minimize(residuals, [0,0], \\\n",
    "        args=(n_stimuli, expdur, m, i+1), \\\n",
    "        method=\"L-BFGS-B\")\n",
    "    # Get the best parameters for the best fit.\n",
    "    C = outcome.x[0]\n",
    "    # Compute the residual sum of squares.\n",
    "    ss_res = residuals(outcome.x, n_stimuli, \\\n",
    "        expdur, m, i+1)\n",
    "    # Compute the total sum of squares.\n",
    "    ss_tot = numpy.sum((m - numpy.mean(m))**2)\n",
    "    # Compute the coefficient of determination.\n",
    "    r_sq = 1.0 - (ss_res / ss_tot)\n",
    "\n",
    "    print(\"Model {}: R squared = {}\".format( \\\n",
    "        i+1, round(r_sq, ndigits=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Bayesian Information Criterion\n",
    "\n",
    "One downside of using $R^{2}$ is that it doesn't distinguish between models that have very few free parameters, and models that have very many. As a rule, the more free parameters in a model, the easier it is to fit it to some data. This is not a feature of how good your model is, but rather of how flexible it is.\n",
    "\n",
    "To counteract this, we can turn to ways in which we can quantify the *goodness of fit*. One of these metrics is the *adjusted R squared*, which is computed from the $R^{2}$, the number of observations ($n$), and the number of free parameters ($k$):\n",
    "\n",
    "$R^{2}_{adjusted} = 1 - {{(1 - R^{2}) (n - 1)} \\over {n - k - 1}}$\n",
    "\n",
    "Another measure for the goodness of fit is the *Bayesian Information Criterion* (BIC). One way of computing the BIC directly uses the residuals:\n",
    "\n",
    "$BIC = n + n \\ln{(2 \\pi)} + n \\ln{({{SS_{res}} \\over {n}})} + \\ln{(n)}(k + 1)$\n",
    "\n",
    "*where $n$ is the number of observations (in this case: number of exposure durations), and $k$ is the number free parameters in a model*\n",
    "\n",
    "Both of these metrics do not only quantify how good a fit is, but also punish models as a function of how many parameters they require.\n",
    "\n",
    "Let's use the BIC to compare our models one last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all models.\n",
    "for i, model in enumerate([model_1, model_2, model_3, model_4]):\n",
    "    # Fit the current model.\n",
    "    outcome = minimize(residuals, [0,0], \\\n",
    "        args=(n_stimuli, expdur, m, i+1), \\\n",
    "        method=\"L-BFGS-B\")\n",
    "    # Get the best parameters for the best fit.\n",
    "    C = outcome.x[0]\n",
    "    # Compute the residual sum of squares.\n",
    "    ss_res = residuals(outcome.x, n_stimuli, \\\n",
    "        expdur, m, i+1)\n",
    "    # Compute the total sum of squares.\n",
    "    ss_tot = numpy.sum((m - numpy.mean(m))**2)\n",
    "    # Compute the coefficient of determination.\n",
    "    r_sq = 1.0 - (ss_res / ss_tot)\n",
    "    \n",
    "    # Count the number of observations.\n",
    "    n = len(expdur)\n",
    "    # Set the number of free parameters.\n",
    "    if i in [0,1]:\n",
    "        # The first two models have one free \n",
    "        # parameter: C\n",
    "        k = 1\n",
    "    elif i in [2,3]:\n",
    "        # The second two models have two free\n",
    "        # parameters: C and t0\n",
    "        k = 2\n",
    "    # Compute the Bayesian Information Criterion.\n",
    "    bic = n + n*numpy.log(2*numpy.pi) + \\\n",
    "        n*numpy.log(ss_res/n) + \\\n",
    "        numpy.log(n)*(k+1)\n",
    "\n",
    "    print(\"Model {}: BIC = {}\".format( \\\n",
    "        i+1, round(bic, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing which model fits best is done by comparison of BICs. By convention, you choose the lowest BIC. This is the best fitting model. You then compute the differences between that BIC and the other models' BICs. The resulting values, ${\\Delta}BIC$, are usually interpreted using the guidelines of Raftery (1995), who considers ${\\Delta}BIC$ values of 2-6 positive evidence in favour of the best fitting model, values of 6-10 as strong evidence, and values over 10 as very stong evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that normally, you would fit a model to each participant, not to the average of the whole sample. You can then compute a $BIC$ for each participant. The sum across all participants of the $BIC$ values for a model is them compared against the other models' sums.\n",
    "\n",
    "Such a procedure would look like this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through models 1 and 3.\n",
    "for i in [0,2]:\n",
    "    # Get the model function for this model.\n",
    "    if i == 0:\n",
    "        model = model_1\n",
    "    elif i == 2:\n",
    "        model = model_3\n",
    "\n",
    "    # Count the number of participants.\n",
    "    n_participants = data.shape[1]\n",
    "\n",
    "    # Maintain a list of BIC values for\n",
    "    # all participants.\n",
    "    bic = numpy.zeros(n_participants, dtype=float)\n",
    "    \n",
    "    # Go through all participants.\n",
    "    for j in range(n_participants):\n",
    "\n",
    "        # Fit the current model.\n",
    "        outcome = minimize(residuals, [0,0], \\\n",
    "            args=(n_stimuli, expdur, m, i+1), \\\n",
    "            method=\"L-BFGS-B\")\n",
    "\n",
    "        # Get the best parameters for the best fit.\n",
    "        C = outcome.x[0]\n",
    "        t0 = outcome.x[1]\n",
    "\n",
    "        # Compute the residual sum of squares.\n",
    "        ss_res = residuals(outcome.x, n_stimuli, \\\n",
    "            expdur, m, i+1)\n",
    "        # Compute the total sum of squares.\n",
    "        ss_tot = numpy.sum((m - numpy.mean(m))**2)\n",
    "        # Compute the coefficient of determination.\n",
    "        r_sq = 1.0 - (ss_res / ss_tot)\n",
    "\n",
    "        # Count the number of observations.\n",
    "        n = len(expdur)\n",
    "        # Set the number of free parameters.\n",
    "        if i in [0,1]:\n",
    "            # The first two models have one free \n",
    "            # parameter: C\n",
    "            k = 1\n",
    "        elif i in [2,3]:\n",
    "            # The second two models have two free\n",
    "            # parameters: C and t0\n",
    "            k = 2\n",
    "        # Compute the Bayesian Information Criterion.\n",
    "        bic[j] = n + n*numpy.log(2*numpy.pi) + \\\n",
    "            n*numpy.log(ss_res/n) + \\\n",
    "            numpy.log(n)*(k+1)\n",
    "\n",
    "    print(\"Model {}: BIC sum = {}\".format( \\\n",
    "        i+1, round(numpy.sum(bic), ndigits=3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
