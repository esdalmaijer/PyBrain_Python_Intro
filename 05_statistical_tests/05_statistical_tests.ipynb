{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for group comparisons\n",
    "\n",
    "In this next bit, we will be thinking about surprising events and group differences. For example, if you read \"men are taller than women\", what does that mean? Are all men taller than all women? Are most men taller than most women? Or maybe men are taller than women *on average*? But what is a meaningful difference? Would it be meaningful if men, on average, are 1 cm taller than women? Where do you draw the line? Also, how many men and women would you need to test before you believe they might have different average heights?\n",
    "\n",
    "How we think about these questions has shaped our usage of statistics, and you could even argue that how we use statistics has shaped how we think about differences between groups. (That sounds a bit esoteric now, but will become clearer towards the end of this section.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you need to measure the entire population?\n",
    "\n",
    "Let's start with an example. When I'm in a debate with someone, but find that I struggle to come up with a sensible argument, I like to hold my breath in anger. My opponent is now left with only two options: Concede the point, or engage in an impromptu breath-holding competition. Obviously, this strategy only works if there is a high probability that I can hold my breath longer than my opponent. How could we come to know this probability?\n",
    "\n",
    "First, we need to know how long I can hold my breath. This is easy: I just hold my breath for as long as I can, and record how long it was using a stopwatch. Let's say this was 83 seconds.\n",
    "\n",
    "Next, we need to compare my breath-holding ability with the people I usually debate. In order to do this, we need to establish who this population is. As I work and live in Cambridge, the people I debate are likely those who live here, and specifically those found in and around the University. This is a problem: It's cumbersome and potentially expensive to test *everyone* who is associated with the University of Cambridge.\n",
    "\n",
    "Instead of the whole target population, maybe we could take a random selection of people from them? If those people are representative, we could use them to infer what the population must look like. This is called a *sample\".\n",
    "\n",
    "In this case, I measured a convenience sample of my Cambridge undergraduate students by by instructing them to hold their breath as long as they could, and write down their time.\n",
    "\n",
    "*Of course, this is not ideal: Undergraduate students are around the same age, and were made to do this as a stupid exercise in what should have been a serious stats class. So perhaps the sample is biased, and the measurement sub-optimal. Let's forget about those very reasonable objections for now, and simply assume the sample is representative of the wider Cambridge population. Or that I only ever debate undergrads, as I'm not brave enough to take on anyone with higher educational credentials than that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# These times were measured during three stats classes.\n",
    "# Each class is in a different list.\n",
    "times_a = [45.53, 41.30, 36.21, 49.3, 41.0, 63, 38.15, \\\n",
    "    35.1, 38.1, 47.9, 46.6, 23.0, 60.0, 42.0, 35.7, 41.2]\n",
    "times_b = [42.0, 38.0, 46.0, 88.1, 125, 41.3, 36.0, \\\n",
    "    40.0, 43.7, 34.7, 38.4, 42.0, 61.0, 36.0, 43.4, \\\n",
    "    36.14, 18.23, 49.0, 38.0, 22.47, 35.51, 63.15, \\\n",
    "    35.2, 51.53, 34.13, 54.0, 43.2]\n",
    "times_c = [94.0, 83.0, 95.0, 36.5, 49.0, 44.0, 43.0, \\\n",
    "    57.0, 35.8, 44.0, 44.4, 49.0, 39.0, 42.0, 35.0, \\\n",
    "    50.0, 53.0, 52.0, 33.0, 37.0, 48.0, 74.0, 41.0, \\\n",
    "    49.0, 38.0, 25.0, 61.0, 47.0, 41.0, 73.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The times were recorded in different lists. Fortunately, lists can easily be combined together. You can simply use the `+` operator, like so:\n",
    "\n",
    "`times = times_a + times_b + times_c`\n",
    "\n",
    "However, you can also use the built-in `extend` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add all the times together.\n",
    "times = []\n",
    "times.extend(times_a)\n",
    "times.extend(times_b)\n",
    "times.extend(times_c)\n",
    "\n",
    "# Now that all the times are combined,\n",
    "# you can create a NumPy array from the\n",
    "# list of all times.\n",
    "t = numpy.array(times, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the times are combined, and put into a single NumPy array, you can create a histogram. You can use the same `histogram` function from NumPy that was used in the examples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram to count how frequently each breath-\n",
    "# holding time occurs in the sample.\n",
    "hist, edges = numpy.histogram(t, bins=20)\n",
    "bin_centres = edges[:-1] + numpy.diff(edges)/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib's `plot` function can be used to draw lines. It needs the x and y values, and you can also pass a line style. Here we pass `-` to create a continuous line. Alternatives include `--` for a dashed line, `-.` for a dash-dot line, and `:` for a dotted line. You can also pass `o` for round markers without a line, or `s` for square markers without a line. Finally, you can combine line and marker characters, for example `-o` for a continuous line with round markers.\n",
    "\n",
    "The `lw` keyword argument sets the **l**ine **w**idth, and the `color` argument the colour. Here we pass the hex value for hot pink, but you can also pass colour names, or even (r,g,b) values (with numbers for each of the red, green, and blue values between 0 and 1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram.\n",
    "pyplot.plot(bin_centres, hist, '-', lw=3, color=\"#FF69B4\")\n",
    "pyplot.ylabel(\"Number of observations\", fontsize=16)\n",
    "pyplot.xlabel(\"Hold-your-breath times (sec)\", fontsize=16)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we know what the population should roughly look like. From the histogram, we can read that the most likely breath-holding times are around 40 seconds. We can also see that the times are quite variable: They range all the way from 20 to 120 seconds! Even if we look at just the times with the most observations, there's a substantial range from about 30-50 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When is a value noticably different?\n",
    "\n",
    "So what about my 83 seconds? Is it a surprising value within this population? More importantly, does it make me likely to win debate-avoiding breath-holding competitions? One way to compute this, is by calculating how many people can hold their breath for less long than me. This proportion would reflect the probability of encountering someone who can hold their breath for less long, and thus of my winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my value.\n",
    "v = 83.0\n",
    "\n",
    "# Plot the histogram.\n",
    "pyplot.plot(bin_centres, hist, '-', lw=3, color=\"#FF69B4\")\n",
    "pyplot.ylabel(\"Number of observations\", fontsize=16)\n",
    "pyplot.xlabel(\"Hold-your-breath times (sec)\", fontsize=16)\n",
    "\n",
    "# Plot my value in there.\n",
    "pyplot.axvline(v, linestyle=\"--\", lw=3, color=\"#FF0000\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation is nice, but we need to compute how many values in the distribution were actually lower than our target value. To do so, we can use the fact that Booleans can be read as 0s and 1s.\n",
    "\n",
    "In order to know which values in `t` are lower than the target value `v`, we can create a Boolean array by using `t < v`. As you might recall from the previous worksheet, this should result in an array with a True value for every value in `t` that was lower than `v`, and False for every value that was not.\n",
    "\n",
    "Because True can be considered 1, and False 0, we can convert a Boolean array to integer values. This can be done using the `astype` function that is built into NumPy arrays. This returns an array with 1 instead of True, and 0 instead of False. The sum of this array is the number of times the original was True. (In this case that is the number of values in `t` that is lower than `v`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute how many values were lower.\n",
    "lower = t < v\n",
    "n_lower = numpy.sum(lower.astype(int))\n",
    "p_lower = float(n_lower) / len(t)\n",
    "\n",
    "print(\"{}/{} observations (prop={}) lower than {} seconds\".format( \\\n",
    "    n_lower, len(t), round(p_lower, ndigits=2), round(v, ndigits=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, about 93% of our sample falls below my time. This means that I would lose my argument only in 7% of cases.\n",
    "\n",
    "Another way of thinking about this is to ask whether my time is surprising given the population. You could argue that my breath-holding ability is somewhat rare, with only 7% of people doing better. You could also argue that it's within the normal population, and that there are quite a lot of people who can hold their breath for much longer. Where you draw this line of what is surprising is arbitrary, and depends on context.\n",
    "\n",
    "What you just calculated is akin to a *p value*. Our measured data forms a *probability distribution* that tells you just how likely each breath-holding time is. When a time is associated with value of p=0.07, it tells you that 93% of a population is expected to score lower.\n",
    "\n",
    "If you would like to play around with distributions and p values some more, this Shiny app is great: https://gallery.shinyapps.io/dist_calc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When is a whole group of values noticably different from what I expected?\n",
    "\n",
    "Imagine that you are an alien, unfamiliar with the human race. You came upon Earth, found a funny-looking mammal, and decided to bring it back to your ship. Unfortunately, you notice too late that humans don't thrive in your water tank, but in fact drown. You grab another one, and observe that they fare better in an environment closer to the earths atmosphere. It turns out they have this weird thing where they suck in air, and then breathe it out again. You assume that this is why they didn't do well in your water tank.\n",
    "\n",
    "You now wonder whether humans have to continuously do this breathing thing, or whether they might have some internal reserves. From your initial water tank mishap, you assume that humans simply cannot do without continuosly inhaling and exhaling air. Hence, your hypothesis is that humans cannot hold their breath. Well, maybe for like 30 seconds, because that's how long your first human visibly struggled in the water tank.\n",
    "\n",
    "You decide to run a study$^*$: You extract a number of Cambridge undergraduates, ask your subjects to stop exhaling new air, and measure how long it takes before they have to inhale again. Of course, you end up with the same data as we've just seen.\n",
    "\n",
    "\\* *You don't have to get IRB approval, because it is known that humans are primitive life forms that probably aren't even conscious. Many even wonder whether they feel pain at all, and even if they do, they probably wouldn't experience it in the same way as you, or even be able to conciously remember it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# These times were measured during three stats classes.\n",
    "times_a = [45.53, 41.30, 36.21, 49.3, 41.0, 63, 38.15, \\\n",
    "    35.1, 38.1, 47.9, 46.6, 23.0, 60.0, 42.0, 35.7, 41.2]\n",
    "times_b = [42.0, 38.0, 46.0, 88.1, 125, 41.3, 36.0, \\\n",
    "    40.0, 43.7, 34.7, 38.4, 42.0, 61.0, 36.0, 43.4, \\\n",
    "    36.14, 18.23, 49.0, 38.0, 22.47, 35.51, 63.15, \\\n",
    "    35.2, 51.53, 34.13, 54.0, 43.2]\n",
    "times_c = [94.0, 83.0, 95.0, 36.5, 49.0, 44.0, 43.0, \\\n",
    "    57.0, 35.8, 44.0, 44.4, 49.0, 39.0, 42.0, 35.0, \\\n",
    "    50.0, 53.0, 52.0, 33.0, 37.0, 48.0, 74.0, 41.0, \\\n",
    "    49.0, 38.0, 25.0, 61.0, 47.0, 41.0, 73.0]\n",
    "\n",
    "# Let's add all the times together.\n",
    "times = []\n",
    "times.extend(times_a)\n",
    "times.extend(times_b)\n",
    "times.extend(times_c)\n",
    "t = numpy.array(times, dtype=float)\n",
    "\n",
    "# Make a histogram to count how frequently each breath-\n",
    "# holding time occurs in the sample.\n",
    "hist, edges = numpy.histogram(t, bins=20)\n",
    "bin_centres = edges[:-1] + numpy.diff(edges)/2.0\n",
    "\n",
    "# Plot the histogram.\n",
    "pyplot.plot(bin_centres, hist, '-', lw=3, color=\"#FF69B4\")\n",
    "pyplot.ylabel(\"Number of observations\", fontsize=16)\n",
    "pyplot.xlabel(\"Hold-your-breath times (sec)\", fontsize=16)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the same code as before, just combined into a single snippet for convenience. Your next question is simple: Is this distribution what you expected? Do the results confirm your hypothesis that humans cannot hold their breath?\n",
    "\n",
    "Your expectation was that humans could not hold their breath, or more specifically that they wouldn't be able to for more than about 30 seconds. The difficulty is in translating your expectation to the language of numbers and distributions.\n",
    "\n",
    "Essentially, what you're saying is that you expect the average human being to be able to hold their breath for a maximum of 30 seconds. That doesn't have to mean that all humans abide by that: some will be worse, and some beter. In terms of a distribution, you're expecting that the central tendency of the distribution is around 30 seconds.\n",
    "\n",
    "Let's plot your expected average in the histogram, and see if it aligns with the real average. We can do this with code that is very similar to the code you used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected average value.\n",
    "m_expected = 30\n",
    "\n",
    "# Compute the real average.\n",
    "m_real = numpy.mean(t)\n",
    "print(\"Expected mean: {}, real mean: {}\".format( \\\n",
    "    round(m_expected, ndigits=2), round(m_real, ndigits=2)))\n",
    "\n",
    "# Plot the histogram.\n",
    "pyplot.plot(bin_centres, hist, '-', lw=3, color=\"#FF69B4\")\n",
    "pyplot.ylabel(\"Number of observations\", fontsize=16)\n",
    "pyplot.xlabel(\"Hold-your-breath times (sec)\", fontsize=16)\n",
    "\n",
    "# Plot the expected and real averages in there.\n",
    "pyplot.axvline(m_expected, linestyle=\"--\", lw=3, color=\"#FF0000\", \\\n",
    "    label=\"hypothesis\")\n",
    "pyplot.axvline(m_real, linestyle=\"--\", lw=3, color=\"#FF69B4\", \\\n",
    "    label=\"data\")\n",
    "pyplot.legend(loc=\"upper right\")\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things of interest in this plot. For starters, your average (hot pink dashed line) doesn't seem to align perfectly with where the distribution is thickest. This is because the distribution is right-skewed, and thus the average might not be the ideal metric for the distributions central tendency. (Using the median would probably be better, but for now we'll just ignore that our data isn't normally distributed.)\n",
    "\n",
    "The next interesting thing is that while a bit of the distribution is below your expected average, the real average is different from what you expected. How can you tell whether this difference is meaningul? This is where statistics come to the rescue.\n",
    "\n",
    "If your expectation of a 30 second average breath-holding time was accurate, you would expect to find that samples that you measure would vary around 30 seconds. Sure, some samples would be slightly higher, and some lower, but over the whole they should sit around 30 seconds. Hence, if you were to run a lot of experiments, the averages that you find in each of them would form a distribution around 30 seconds. This distribution is called a *null distribution*.\n",
    "\n",
    "The above means that there should be a relatively low chance that you find an average value that is a lot higher than 30 seconds. Therefore, finding such a value would be quite surprising. In fact, finding a few surprising results might even persuade you to abandon your initial hypothesis of a 30 second average, and instead lead you to believe that the average is higher. This is the core principle in *null-hypothesis significance testing*.\n",
    "\n",
    "There is still one important question we haven't answered: When is a value considered \"higher\"? Specifically, you measured an average of about 47 seconds. This is obviously higher than the expected 30 seconds, but how can you tell whether it is meaningfully higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "This is where the concept of *variance* comes in. Variance is how much measurements in a sample deviate from its mean. To compute it, you simply take the difference between each measurement and the mean. Unfortunately, when you average those differences, you should end up at 0, because there will be as many positive as there are negative differences (assuming the data is normally distributed). To deal with this, you could simply square all differences between each measurement and the mean. In that way, all differences are positive, so you can safely average them. (Of course, squaring all values will make them all super big, so afterwards you could take the square root of the average of squared differences, so that you end up with a reasonable value again.) The higher this value, the higher variance.\n",
    "\n",
    "This idea is captured in the *standard deviation*, which is essentially the average unsigned difference between all measurments and their average. It's computed like this:\n",
    "\n",
    "$s(x) = \\sqrt{{{ \\Sigma^{n}_{i=1} (x_{i} - \\bar{x})^{2} } \\over {n}}}$\n",
    "\n",
    "In this equation, $x$ is all our measurements, $n$ is our sample size, and $\\bar{x}$ is the average of our measurement.\n",
    "\n",
    "The neat thing about variance is that it gives us an idea of the context of an observed difference. Take, for example, a breath-holding time of 67 seconds (20 seconds above the average): If measurements on average deviated 100 seconds from the average, then 20 seconds doesn't sound like a very big value. (It's only 0.2 standard deviations.) However, if they only deviated by 2 seconds, 20 seconds seems like a pretty unexpected value! (That would be 10 standard deviations!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard error\n",
    "\n",
    "The average and standard deviation are *population characteristics*. They reflect the central tendency and spread of values in a population. Imagine the breath-holding abilities of all people on earth. The more people you test, the closer your estimation of the average and standard deviation is going to be to the real population, up until the point that you've tested every single member of the population.\n",
    "\n",
    "Imagine that you, the alien who's super interested in how long humans can hold their breath, do a whole bunch of studies. In each, you take a sample of humans, and measure the samples average breath-holding ability. You won't get the same mean every time, but because the humans all come from the same population of \"humans on earth\", it's more likely that you will find means closer to the population mean. In other words, the means that you measure in each of your studies will form their own distribution.\n",
    "\n",
    "Let's illustrate this point in your own sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random number generator (this makes\n",
    "# sure that we get the same result when re-running\n",
    "# this code, which is not necessary, but good\n",
    "# for in-class demo purposes.)\n",
    "numpy.random.seed(3)\n",
    "\n",
    "# Define the number of sub-samples.\n",
    "n_samples = 100\n",
    "\n",
    "# Count the number of observations you had in\n",
    "# your original measurement.\n",
    "n = len(t)\n",
    "\n",
    "# Keep a record of the means that you find for\n",
    "# each sub-sample.\n",
    "m = numpy.zeros(n_samples, dtype=float)\n",
    "# Run through all \"studies\"\n",
    "for i in range(n_samples):\n",
    "    # Choose a random sub-sample of 10 people\n",
    "    # from the measured times.\n",
    "    sub_sample = numpy.random.choice(n, 10)\n",
    "    # Compute the average of this sub-sample.\n",
    "    m[i] = numpy.mean(t[sub_sample])\n",
    "\n",
    "# Make a histogram to count how frequently each breath-\n",
    "# holding average time occured in the sub-samples.\n",
    "hist, edges = numpy.histogram(m, bins=20)\n",
    "bin_centres = edges[:-1] + numpy.diff(edges)/2.0\n",
    "\n",
    "# Plot the histogram.\n",
    "pyplot.plot(bin_centres, hist, '-', lw=3, color=\"#FF0000\")\n",
    "pyplot.ylabel(\"Number of observations\", fontsize=16)\n",
    "pyplot.xlabel(\"Sub-sample breath-holding average (sec)\", fontsize=16)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above illustrates how the sample means would be distributed if you had taken samples of 10 people from your \"population\" of your actual sample of 73 undergraduate students. As you can see, the sub-sample averages are centred around the population mean of 47 seconds.\n",
    "\n",
    "You can use your new knowledge of this sample mean distribution to compute your expected measurement error. This is known as the *standard error of the mean*, and gives you an idea of how well your sample reflects the true population mean. You compute it by dividing the standard deviation (computed above) by the square root of the number of observations.\n",
    "\n",
    "$s(\\bar{x}) = {{s} \\over {\\sqrt{n}}}$\n",
    "\n",
    "This equation tells you that the more people are in your sample, i.e. the higher $n$ is, the lower your standard error is going to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-sample t-test\n",
    "\n",
    "For this next trick, you will use variance to quantify how much you believe in your observed difference. Specifically, you're going to take the breath-holding difference (between your expected value and the observed average), and you'll divide that by the standard error of the mean.\n",
    "\n",
    "This is a sensible thing to do: You observed a particular difference (signal), and now you're putting that difference into the perspective of expected measurement error (noise). In a way, you're just trying to figure out whether your observed difference is a product of the noise in your measurement, or whether it might actually be because your hypothesis was wrong.\n",
    "\n",
    "$t = {{\\bar{x} - \\mu_{0}} \\over {s_{\\bar{x}}}}$\n",
    "\n",
    "The numerator here is your observed average (47 seconds) minus your expected average (30 seconds). In other words, this is the difference between your measurement and your null hypothesis. If your null hypothesis is true, you would expect this value to be 0.\n",
    "\n",
    "The denominator is your \"noise\". The higher the standard error of the mean, the lower your confidence in your measured average is, and hence the lower your confidence in its difference from the null hypothesis. More noise directly translates into a lower $t$ value.\n",
    "\n",
    "So let's see all of this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your expected average.\n",
    "m_null = 30.0\n",
    "\n",
    "# Count the number of observations.\n",
    "n = len(t)\n",
    "\n",
    "# Compute the average.\n",
    "m = numpy.mean(t)\n",
    "\n",
    "# Compute the standard deviation.\n",
    "sd = numpy.sqrt(numpy.sum((t - m)**2) / (n))\n",
    "\n",
    "# Compute the standard error of the mean.\n",
    "sem = sd / numpy.sqrt(n-1)\n",
    "\n",
    "# Now compute the t-value.\n",
    "t_val = (m - m_null) / sem\n",
    "\n",
    "print(\"m={}, sd={}, sem={}, t={}\".format( \\\n",
    "    round(m, ndigits=2), \\\n",
    "    round(sd, ndigits=2), \\\n",
    "    round(sem, ndigits=2), \\\n",
    "    round(t_val, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The $t$-value is over 8!\n",
    "\n",
    "But... Is that a lot?\n",
    "\n",
    "Remember, we expect a $t$ value of 0 if the null hypothesis is true. We also expect a low $t$ value if the measurement noise is high. In order to know whether this $t$ is surprising, we need to know what the $t$ distribution looks like. (You can also mess around with an interactive $t$ distribution here: https://rpsychologist.com/d3/tdist/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# Create x values.\n",
    "x = numpy.arange(-10.0, 10.01, 0.01)\n",
    "# Create t values, based on the degrees of freedom.\n",
    "t_dist = scipy.stats.t.pdf(x, df=n-1)\n",
    "\n",
    "# Plot the t distribution.\n",
    "pyplot.plot(x, t_dist, '-', lw=3, color=\"#009900\")\n",
    "pyplot.xlabel(\"Possible t values\", fontsize=16)\n",
    "pyplot.ylabel(\"Probability density\", fontsize=16)\n",
    "# Plot our t value in the plot.\n",
    "pyplot.axvline(t_val, linestyle=\"--\", lw=3, color=\"#FF69B4\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the $t$ value that you observed (pink dashed line) is not very likely at all given the distrubution of $t$ values! You can compute exactly how likely it is that a $t$ value is equal to or higher than yours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = 1.0 - scipy.stats.t.cdf(t_val, df=n-1)\n",
    "\n",
    "print(\"One-sided p value = {}\".format(p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is testing a *directional hypothesis*: Whether the observed $t$ value is higher than expected given the $t$ distribution. However, sometimes you might not have a directional hypothesis, but simply wonder whether an observed value is different from your expectation, whether that is higher or lower. You can then compute how likely it is to observe a $t$ value of higher or equal to the absolute $|t|$ value, or equal to or lower than $-|t|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = 2.0 * (1.0 - scipy.stats.t.cdf(numpy.abs(t_val), df=n-1))\n",
    "\n",
    "print(\"Two-sided p value = {}\".format(p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be delighted to know that we can simplify the above code by using existing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# Define your expected average.\n",
    "m_null = 30.0\n",
    "\n",
    "# Count the number of observations.\n",
    "n = len(t)\n",
    "\n",
    "# Compute the average.\n",
    "m = numpy.mean(t)\n",
    "\n",
    "# Compute the standard deviation.\n",
    "sd = numpy.std(t)\n",
    "\n",
    "# Compute the standard error of the mean.\n",
    "sem = sd / numpy.sqrt(n)\n",
    "\n",
    "# Now compute the t-value.\n",
    "t_val, p_val = ttest_1samp(t, m_null)\n",
    "\n",
    "print(\"m={}, sd={}, sem={}, t={}, p={}\".format( \\\n",
    "    round(m, ndigits=2), \\\n",
    "    round(sd, ndigits=2), \\\n",
    "    round(sem, ndigits=2), \\\n",
    "    round(t_val, ndigits=2), \\\n",
    "    p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this p value mean?\n",
    "\n",
    "The $p$ value that you just computed is the probability of you finding an average breath-holding time that is equal or more extreme than 47 seconds, *if the null hypothesis is true*.\n",
    "\n",
    "This is important, so I'm going to stress it: *If your expected average of 30 seconds is the real population average*, the estimated probability of you measuring a 47 second average in your sample of 73 people is $3.26e-12$.\n",
    "\n",
    "This could mean two things:\n",
    "\n",
    "1) This is a fluke, the true population average is 30 seconds, and you just happened to test a sample of people with a higher average. Random chance. It's unlikely, but it happens.\n",
    "\n",
    "2) Maybe your expected average of 30 seconds isn't the true population mean.\n",
    "\n",
    "How do you known which of these two options is correct? Simple: You don't! Not on the basis of a single study, anyways. You could do more studies, and compute $p$ values for all those studies too. If the null hypothesis (\"humans can hold their breath for 30 seconds on average\") is true, the $p$ values should be uniformly distributed between 0 and 1. If the null hypothesis is not true, you would expect $p$ values to be skewed towards 0.\n",
    "\n",
    "For a cool demo on the $p$ value distribution, see here: https://rpsychologist.com/d3/pdist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences between two measurements within the same individuals\n",
    "\n",
    "Before now, you had a very specific hypothesis based on some pilot data (N = 1 accidentally drowned human in a water tank after 30 seconds). Now that you've found a group of humans with a higher average breath-holding ability, you consider the possibility that maybe they got better at holding their over time.\n",
    "\n",
    "Being able to train humans to hold their breath for long times would be great for you: It would mean you could stick them in water tanks to do all the necessary experiments, without risking they die before your experiments finish. You thus set up a new experiment: You'll take your existing sample of humans, and you make them hold their breath for as long as they can for five times a day. You also make them run around your space ship a lot$^*$.\n",
    "\n",
    "*\\*This is not because you think it will increase their lung capacity, but because they keep trying to escape their enclosure.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "You now have data on how long your sample could hold their breath at the time of first measurement, and also data from a measurement several weeks from the first. These times are stored in the file `breath_times.csv`.\n",
    "\n",
    "The extension `.csv` stands for \"comma-separated values\", and they can be loaded into Python. NumPy's `loadtxt` function can read data directly from files that contain data that is separated (delimited) by commas, tabs, or other delimiters. To use this function, you need to specify the path to the file that you're trying to load. In addition, you can specify the delimiter (in this case `,`), the type of data (here it will be `float` for floating-point numbers), and whether or not the data should be transposed (via the `unpack` argument).\n",
    "\n",
    "The function will return a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data.\n",
    "raw_data = numpy.loadtxt(\"breath_times.csv\", \\\n",
    "    delimiter=\",\", dtype=float, unpack=True)\n",
    "\n",
    "# Convenience renaming.\n",
    "x1 = raw_data[0,:]\n",
    "x2 = raw_data[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your question is simple: Is the average breathing time different at the second time? If so, humans might be made to improve, and that would be wonderful for your water tank experiments!\n",
    "\n",
    "Let's start with the basics: Compute the difference between the first and second measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference.\n",
    "difference = x2 - x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive values reflect a longer breath-holding time at the second measurement (potential improvement), whereas negative values reflect a longer breath-holding time at the first measurement (potential decline).\n",
    "\n",
    "Let's compute the mean, standard deviation, and standard error of the mean for these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of observations.\n",
    "n = len(difference)\n",
    "\n",
    "# Compute the average.\n",
    "m = numpy.mean(difference)\n",
    "\n",
    "# Compute the standard deviation.\n",
    "sd = numpy.sqrt(numpy.sum((difference - m)**2) / (n))\n",
    "\n",
    "# Compute the standard error of the mean.\n",
    "sem = sd / numpy.sqrt(n-1)\n",
    "\n",
    "print(\"m={}, sd={}, sem={}\".format( \\\n",
    "    round(m, ndigits=2), \\\n",
    "    round(sd, ndigits=2), \\\n",
    "    round(sem, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, your null hypothesis is that humans can't improve. Thus, your expected difference is 0 milliseconds.\n",
    "\n",
    "Because you have this specific prediction, you can compute a $t$ value again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your expected average.\n",
    "m_null = 0.0\n",
    "\n",
    "# Now compute the t-value.\n",
    "t_val = (m - m_null) / sem\n",
    "\n",
    "print(\"m={}, sd={}, sem={}, t={}\".format( \\\n",
    "    round(m, ndigits=2), \\\n",
    "    round(sd, ndigits=2), \\\n",
    "    round(sem, ndigits=2), \\\n",
    "    round(t_val, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means you can see where the $t$ value lies on the expected probability distribution, and compute a $p$ value from that point on the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x values.\n",
    "x = numpy.arange(-10.0, 10.01, 0.01)\n",
    "# Create t values, based on the degrees of freedom.\n",
    "t_dist = scipy.stats.t.pdf(x, df=n-1)\n",
    "\n",
    "# Plot the t distribution.\n",
    "pyplot.plot(x, t_dist, '-', lw=3, color=\"#009900\")\n",
    "pyplot.xlabel(\"Possible t values\", fontsize=16)\n",
    "pyplot.ylabel(\"Probability density\", fontsize=16)\n",
    "# Plot our t value in the plot.\n",
    "pyplot.axvline(t_val, linestyle=\"--\", lw=3, color=\"#FF69B4\")\n",
    "pyplot.show()\n",
    "\n",
    "# Compute the p value.\n",
    "p_val = 2.0 * (1.0 - scipy.stats.t.cdf(numpy.abs(t_val), df=n-1))\n",
    "\n",
    "print(\"Two-sided p value = {}\".format(p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related-samples t-test\n",
    "\n",
    "What you've just done, is called a *related-samples t-test*. It's used to test for differences between two measurements *within participants*. The null hypothesis is that there is no difference between the two related measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp, ttest_rel\n",
    "\n",
    "# One-sample t-test of the difference between two related measurements.\n",
    "t_val, p_val = ttest_1samp(x2-x1, 0)\n",
    "print(\"One-sample t-test:      t={}, p={}\".format( \\\n",
    "    round(t_val, ndigits=2), p_val))\n",
    "\n",
    "# Related-samples t-test of the difference between two related measurements.\n",
    "t_val, p_val = ttest_rel(x2, x1)\n",
    "print(\"Related-samples t-test: t={}, p={}\".format( \\\n",
    "    round(t_val, ndigits=2), p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in a measurement in two different groups\n",
    "\n",
    "You, at this point more-or-less accidentally the galaxy's foremost expert on human breathing, decide that it's too much of a hassle to train humans to hold their breath for longer. Instead, you decide to investigate whether certain subsets of humans might have longer breath-holding times. These would be ideal water-tank inhabitants!\n",
    "\n",
    "You decide to compare your existing sample with an entirely different group of humans who you think might be better at holding their breath. You reason that this second group of humans spends a lot more time in the water, so they must be better at holding their breath. In fact, they seem to have some adaptations that other humans lack, including a dorsal fin and a single hindleg that seems to propel them in the water.\n",
    "\n",
    "Have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Load the raw data.\n",
    "raw_data = numpy.loadtxt(\"breath_times_between-subjects.csv\", \\\n",
    "    delimiter=\",\", dtype=float, unpack=True)\n",
    "\n",
    "# Convenience renaming.\n",
    "group_1 = raw_data[0,:]\n",
    "group_2 = raw_data[1,:]\n",
    "\n",
    "# Set some colours for the groups.\n",
    "colours = [\"#FF69B4\", \"#AAAAAA\"]\n",
    "\n",
    "# Loop through the groups.\n",
    "for i, group_n in enumerate([group_1, group_2]):\n",
    "\n",
    "    # Compute the real average.\n",
    "    m_real = numpy.mean(group_n)\n",
    "\n",
    "    # Make a histogram to count how frequently each breath-\n",
    "    # holding time occurs in the sample.\n",
    "    hist, edges = numpy.histogram(group_n, bins=10)\n",
    "    bin_centres = edges[:-1] + numpy.diff(edges)/2.0\n",
    "\n",
    "    # Plot the histogram.\n",
    "    pyplot.plot(bin_centres, hist, '-', lw=3, color=colours[i], \\\n",
    "        label=\"Group {}\".format(i+1))\n",
    "\n",
    "    # Plot the expected and real averages in there.\n",
    "    pyplot.axvline(m_real, linestyle=\"--\", lw=3, color=colours[i])\n",
    "\n",
    "# Finish the plot.\n",
    "pyplot.ylabel(\"Number of observations\", fontsize=16)\n",
    "pyplot.xlabel(\"Hold-your-breath times (sec)\", fontsize=16)\n",
    "pyplot.legend(loc=\"upper right\")\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear that these two groups are very different. However, you'd still like to get some statistical validation. Unfortunately, you can't use a related-samples t-test, as that was for measurements within the same individuals. Here, there are two groups of different individuals, so you can't simply compute difference scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent samples t-test\n",
    "\n",
    "In a paired-samples t-test, you computed the differences between pairs of data (measurements collected within an individual). You then computed the average and the variance of this difference, and used those in your assessment of whether the two groups of measurements were different. For two independent groups, the data is not paired in the same way. This makes it impossible to use the same approach.\n",
    "\n",
    "You can still compute the average and variance in each group. This means you could still compute the numerator in a t-test as the difference between the group means:\n",
    "\n",
    "${\\bar{x_{1}} - \\bar{x_{2}}}$\n",
    "\n",
    "Previously, the denominator was the standard error of the mean: The standard divided by the square root of the number of samples. Here, however, we have *two* standard deviations; one for each group. We need to combine those into the *pooled standard deviation* before we can compute a $t$ value:\n",
    "\n",
    "$s_{pooled} = \\sqrt{ {s^{2}_{x_{1}} + s^{2}_{x_{2}}} \\over {2} }$\n",
    "\n",
    "And now you can compute a $t$ value!\n",
    "\n",
    "$t = { {\\bar{x_{1}} - \\bar{x_{2}}} \\over {s_{pooled} \\sqrt{{2 \\over n}}} }$\n",
    "\n",
    "Or, in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t, p = ttest_ind(group_2, group_1)\n",
    "\n",
    "print(\"t = {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for $t$ above is only applicable when two assumptions are met: The groups must be of equal size, and the groups must be of equal variance.\n",
    "\n",
    "If the groups are not of equal size, the pooled standard deviation instead becomes a weighted average of the two independent standard deviations.\n",
    "\n",
    "If the groups are of unequal variance, things get even more complicated. The denominator is caluclated differently, and the degrees of freedom (used to construct a $t$ distribution) are computed in a different way too.\n",
    "\n",
    "The best thing to do here, is use *Welch's test*. This is a modified independent-samples t-test that can handle unequal group sizes and variances. In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = ttest_ind(group_2, group_1, equal_var=False)\n",
    "\n",
    "print(\"t = {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for correlation\n",
    "\n",
    "People who listen to Taylor Swift tend to be happier. Across the population, people who indicate they listen to Taylor Swift for longer, also rate themselves as being happier<sup>\\*</sup>. In other words, happiness and Taylor Swift listening *covary*. This can be expressed as a correlation: a quantification of how much two variables relate to each other.\n",
    "\n",
    "<small>\\*</small><sub><sup>Note: Not actually based on peer-reviewed research.</sup><sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the concept of correlation, let's turn to a dataset. We asked a bunch of Cambridge students<sup>\\*</sup> how often they listen to Taylor Swift (minutes per day), and how happy they would rate themselves on a scale of 0-10. We also recorded what year they were in. We then divided the dataset into 6 groups: undergrad years 1-4, MPhil or equivalent, and PhD. We then sorted the data to match individuals across the 6 groups who reported the exact same listening time. Miraculously, the groups matched *perfectly*. The data are stored in the attached `happy_taytay.csv` file. In this file, each row represents individual participants with the exact same daily Swift listening time. Each column represents a variable: the first is daily Swift listening time, the second is the happiness rating of 1st Year undergrads, the third is the happiness rating of 2nd Year students, etc. until we reach the happiness rating of PhD students in the final column.\n",
    "\n",
    "<small>\\*</small><sub><sup>No, we didn't. We just made up some data.</sup><sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset into Python first. We can do this via a helpful NumPy function called `loadtxt`. This function requires us to pass the path to the dataset (its name and location on your computer or wherefrom else you're loading it), and a few keyword arguments that determine how the data is loaded. The first of these is `dtype`, which specifies what type of data is in our file: floating point numbers, or floats. The second is `delimiter`, which specifies how values are separated from each other: by commas. The third is `skiprows`, which indicates how many rows should be skipped while loading the data: just one row, as this contains the header data (with variable names). Finally, there is `unpack`, which determines whether data is transposed or not: In our case it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries that we'll use. These are again NumPy\n",
    "# and Matplotlib.\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Load the data from the attached data file.\n",
    "data = numpy.loadtxt(\"happy_taytay.csv\", dtype=float, \\\n",
    "    delimiter=\",\", skiprows=1, unpack=True)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, we can do some convenience renaming of variables. We will use `tay_minutes` for the number of minutes participants listen to Taylor Swift every day. We will use `happy_*` for the average happiness rating for each group, where the `*` indicates what group the happiness rating belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable names that are a bit easier to use.\n",
    "# The amount of minutes each student has listened to Taylor \n",
    "# Swift is in the first row (at index 0!), and is spread\n",
    "# across all columns (\"all\" is indicated by \":\").\n",
    "tay_minutes = data[0,:]\n",
    "# The happiness data is in the second (at index 1!) until \n",
    "# the seventh (at index 6!) row.\n",
    "happy_Y1 = data[1,:]\n",
    "happy_Y2 = data[2,:]\n",
    "happy_Y3 = data[3,:]\n",
    "happy_Y4 = data[4,:]\n",
    "happy_M = data[5,:]\n",
    "happy_PhD = data[6,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for what the data look like, let's plot all of the values in different sub-plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a new figure with a reasonably large size. \n",
    "# This size is indicated in inches, and the dpi argument\n",
    "# sets the number of pixels per inch.\n",
    "fig, ax = pyplot.subplots(nrows=2, ncols=3, \\\n",
    "    figsize=(32,12), dpi=100.0, sharex=True, sharey=True)\n",
    "\n",
    "# First row, first column.\n",
    "ax[0][0].set_title(\"Year 1\", fontsize=24)\n",
    "ax[0][0].plot(tay_minutes, happy_Y1, 'o')\n",
    "# First row, second column.\n",
    "ax[0][1].set_title(\"Year 2\", fontsize=24)\n",
    "ax[0][1].plot(tay_minutes, happy_Y2, 'o')\n",
    "# First row, third column.\n",
    "ax[0][2].set_title(\"Year 3\", fontsize=24)\n",
    "ax[0][2].plot(tay_minutes, happy_Y3, 'o')\n",
    "# Second row, first column.\n",
    "ax[1][0].set_title(\"Year 4\", fontsize=24)\n",
    "ax[1][0].plot(tay_minutes, happy_Y4, 'o')\n",
    "# Second row, second column.\n",
    "ax[1][1].set_title(\"Master\", fontsize=24)\n",
    "ax[1][1].plot(tay_minutes, happy_M, 'o')\n",
    "# Second row, third column.\n",
    "ax[1][2].set_title(\"PhD\", fontsize=24)\n",
    "ax[1][2].plot(tay_minutes, happy_PhD, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which of the years do you think happiness relates to listening to Taylor Swift? In other words, in what year do you think the amount of minutes that student listens to Taylor Swift every day correlates with their happiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to guess the degree of correlation in each plot by just eyeballing them. A more scientific approach would be to quantify the correlation in each plot. The most common correlation quantification is Pearson's correlation coefficient.\n",
    "\n",
    "You can compute Pearson's coefficient manually (and we will later), but you can also just use an existing function from the Scientific Python package (SciPy). SciPy has a statistics sub-module, which has a `pearsonr` function. Let's load that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pearsonr` function returns two values: R and p. R is Pearson's correlation coefficient, and p is the associated p-value (which you've learned about in earlier practicals). We'll use the R, and ignore the p for now. Let's redraw the plots, but now add a correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure to plot data in. This is the same as \n",
    "# before.\n",
    "fig, ax = pyplot.subplots(nrows=2, ncols=3, \\\n",
    "    figsize=(32,12), dpi=100.0, sharex=True, sharey=True)\n",
    "\n",
    "# Now use a for-loop to go through all variables.\n",
    "# NOTE: The following creates two variable that change\n",
    "# on every iteration of the for loop:\n",
    "#     i is the iteration number, starting at 0 and\n",
    "#             going up by 1 on every iteration.\n",
    "#     happy is the value of each variable in the \n",
    "#             list that is passed to the enumerate function.\n",
    "for i, happy in enumerate([happy_Y1, happy_Y2, \\\n",
    "    happy_Y3, happy_Y4, happy_M, happy_PhD]):\n",
    "    \n",
    "    # Determine which row we should draw in. This is the \n",
    "    # integer division (ignoring any remainder) of i and\n",
    "    # the number of columns.\n",
    "    row = i // 3\n",
    "    # Determine which column we should draw in. This is\n",
    "    # the remainder after division of i and the number of\n",
    "    # columns.\n",
    "    col = i % 3\n",
    "    \n",
    "    # Compute the Pearson correlation for Swift listening\n",
    "    # and the current group's happiness.\n",
    "    r, p = pearsonr(tay_minutes, happy)\n",
    "    \n",
    "    # Create a label for the current group, including the\n",
    "    # Pearson R.\n",
    "    label = \"R = %.2f\" % (r)\n",
    "    \n",
    "    # Draw the data.\n",
    "    ax[row][col].plot(tay_minutes, happy, 'o', label=label)\n",
    "\n",
    "    # Draw a legend in the graph.\n",
    "    ax[row][col].legend(loc=\"upper left\", fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's correlation coefficient can vary between -1 (complete negative correlation), 0 (no correlation at all), and 1 (complete positive correlation). In the above figure, the top-left plot shows no correlation, whereas the bottom-right plot shows a near-perfect positive correlation.\n",
    "\n",
    "From this, you could conclude that listening to Taylor Swift has no relation with happiness in Year 1 undergraduates, and almost a 1:1 relation with happiness in PhD students.\n",
    "\n",
    "One interpretation for this result could be that listening to Taylor Swift determines a great deal of PhD students' happiness, but an equally supported interpretation would be that PhD students who are very happy listen to Taylor Swift more often. Alternatively, there could be a third factor that determines both happiness and Taylor Swift listening. For example, maybe having super peppy friends increases both PhD students' happiness *and* the number of minutes they listen to Taylor Swift every day. In sum, **correlation does not imply causation**, it merely implies the existence of a direct or indirect relation. (And sometimes a correlation is entirely coincidental!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Important**: Variance and Co-variance\n",
    "\n",
    "Before you go on to learn ALL the things about correlation coefficients, it's good to get (re-)acquainted with two important concepts: *variance* and *covariance*.\n",
    "\n",
    "Let's say we have a variable $x$, and it consists of the following numbers: $[9, 17, 11, 6, 4]$ There are 5 observations in $x$, and the mean $\\bar{x}$ is 9.4\n",
    "\n",
    "Let's say we have another variable $y$, and it consists of the following numbers: $[18, 10, 20, 14, 6]$ There are 5 observations in $y$, and the mean $\\bar{y}$ is 13.6\n",
    "\n",
    "\n",
    "**Variance** indicates how much individual observations deviate from the mean. To compute it, you follow these steps: \n",
    "\n",
    "1. Subtract the mean from every value (results in both positive and negative deviations from the mean).\n",
    "2. Square those numbers to get rid of their sign (results in values that are higher the further away each observation was from the mean, in either direction)\n",
    "3. Sum all the values.\n",
    "4. Take the square root of the sum.\n",
    "\n",
    "Or, in an equation:\n",
    "\n",
    "$var(x) = \\sqrt{\\Sigma^{n}_{i=1} (x_{i} - \\bar{x})^{2}}$\n",
    "\n",
    "Numerical example:\n",
    "\n",
    "$var(x) = \\sqrt{\\Sigma^{5}_{i=1} (x_{i} - 9.4)^{2}}$\n",
    "\n",
    "$var(x) = \\sqrt{\\Sigma [(9-9.4)^{2}, (17-9.4)^{2}, (11-9.4)^{2}, (6-9.4)^{2}, (4-9.4)^{2}] }$\n",
    "\n",
    "$var(x) = \\sqrt{\\Sigma [-0.4^{2},  7.6^{2},  1.6^{2}, -3.4^{2}, -5.4^{2}]}$\n",
    "\n",
    "$var(x) \\approx \\sqrt{\\Sigma [0.16, 57.76,  2.56, 11.56, 29.16]}$\n",
    "\n",
    "$var(x) \\approx \\sqrt{101.2}$\n",
    "\n",
    "$var(x) \\approx 10.06$\n",
    "\n",
    "The higher the variance is, the more individual observations deviate from the mean of all observations.\n",
    "\n",
    "**Co-variance** indicates to what extend observations in two variables deviate from the mean *at the same time*. In other words: covariance quantifies whether x rises when y rises as well. To compute it, follow these steps:\n",
    "\n",
    "1. Subtract the mean of $x$ from every sample in $x$. (High positive or negative values indicate a large difference from the mean.)\n",
    "2. Subtract the mean of $y$ from every sample in $y$. (High positive or negative values indicate a large difference from the mean.)\n",
    "3. Multiply the values found in steps a and b. You now have one value for each observation. (High positive values indicate an observation was positively or negatively different from the mean in both $x$ and $y$; whereas high negative values indicate that an observation was positively different from the mean in $x$ but negatively in $y$, or vice versa.)\n",
    "4. Sum all values obtained in step 3. (A gigh positive sum indicates that many observations were different from the mean in the same direction in both $x$ and $y$, and a high negative sum indicates that many observations were different from the mean in opposite directions between $x$ and $y$. Importantly, a sum that is closer to zero indicates that observations in $x$ and $y$ were not systematically different in the same or the opposite direction between $x$ and $y$.)\n",
    "5. (Divide the value computed in step 4 by the number of observations. This makes the resulting value more analogous to the standard deviation, which is the variance divided by the number of observations.)\n",
    "\n",
    "Or, in an equation:\n",
    "\n",
    "$cov(x,y) = {{1} \\over {n}} \\Sigma^{n}_{i=1} (x_{i} - \\bar{x}) (y_{i} - \\bar{y})$\n",
    "\n",
    "Numerical example:\n",
    "\n",
    "$cov(x,y) = {{1} \\over {5}} \\Sigma^{5}_{i=1} (x_{i} - 9.4) (y_{i} - 13.6)$\n",
    "\n",
    "$cov(x,y) = 0.2 \\Sigma [(9-9.4)*(18-13.6), (17-9.4)*(10-13.6), (11-9.4)*(20-13.6), (6-9.4)*(14-13.6), (4-9.4)*(6-13.6)]$\n",
    "\n",
    "$cov(x,y) = 0.2 \\Sigma [-0.4*4.4, 7.6*-3.6, 1.6*6.4, -3.4*0.4, -5.4*-7.6]$\n",
    "\n",
    "$cov(x,y) \\approx 0.2 \\Sigma [ -1.76, -27.36,  10.24,  -1.36,  41.04]$\n",
    "\n",
    "$cov(x,y) \\approx 0.2 * 20.8$\n",
    "\n",
    "$cov(x,y) \\approx 4.16$\n",
    "\n",
    "**Correlation** is the ratio between the covariance of two variables and their respective variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this correlation magic, and how can I do it myself?!\n",
    "\n",
    "At this point, Pearson's correlation coefficient (also known as Pearson's R) will probably look like a bit of a black box. To give you some more insight into how it works, let's examine the coefficient a bit further. In short, Pearson's R is calculated by dividing two populations' covariance by the product of their standard deviations. Or, in a formula:\n",
    "\n",
    "$\\rho_{X,Y} = {{cov(X,Y)} \\over {\\sigma_{X}\\sigma_{Y}}}$\n",
    "\n",
    "For a sample, the formula looks considerably more scary:\n",
    "\n",
    "$r_{x,y} = {{\\Sigma^{n}_{i=1} (x_{i} - \\bar{x}) (y_{i} - \\bar{y})} \\over {\\sqrt{\\Sigma^{n}_{i=1} (x_{i} - \\bar{x})^{2}} \\sqrt{\\Sigma^{n}_{i=1} (y_{i} - \\bar{y})^{2}}}}$\n",
    "\n",
    "Although your initial reaction might be to blindly glance over the equations, it might be good to think about what they mean. Let's break them down into smaller parts.\n",
    "\n",
    "First, $x$ and $y$ represent variables. For example, $x$ could be \"minutes listened to Taylor Swift per day\", and $y$ could be \"self-reported happiness rating\".\n",
    "\n",
    "Probably the most simple bit of the equation is $x_{i} - \\bar{x}$. This simply means \"subtract the average value of x ( $\\bar{x}$ ) from the current value of x ( $x_{i}$ )\".\n",
    "\n",
    "Why does it say \"the current value of x\"? Well, that's because the big sigma ($\\Sigma$) is a *sum operator*. It goes through all values of the variable $x$, does something with them (defined after the sigma), and then adds up all outcomes. You might have noticed the $n$ in $\\Sigma^{n}$: that simply means \"the number of values in the variable x\". (For example, 1000 students' Taylor Swift listening minutes.) Finally, there is the $i=1$ bit in $\\Sigma^{n}_{i=1}$. This means \"start at the first observation\". So, in sum, the sum operator goes through all observations, does something to them, and then adds the outcomes up in one big total value.\n",
    "\n",
    "Saying the sum operator \"does something\" to the values is a bit vague. Here, we have a sum operator above the line (the numerator) that reads $\\Sigma^{n}_{i=1} (x_{i} - \\bar{x}) (y_{i} - \\bar{y})$ Here, the \"does something\" means is $(x_{i} - \\bar{x}) (y_{i} - \\bar{y})$, which means \"multiply $x_{i} - \\bar{x}$ with $y_{i} - \\bar{y}$\". We had already established that \"$x_{i} - \\bar{x}$\" means \"subtract the average of x from the current value of x\".\n",
    "\n",
    "We can write this in Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take Taylor Swift listening as 'x' values.\n",
    "x = tay_minutes\n",
    "mean_x = numpy.mean(x)\n",
    "# Let's take the happiness ratings of Year 3 undergrads \n",
    "# as 'y' values.\n",
    "y = happy_Y3\n",
    "mean_y = numpy.mean(y)\n",
    "\n",
    "# Count the number of observations we have.\n",
    "n = len(x)\n",
    "\n",
    "# Start with a sum value of 0.\n",
    "s = 0.0\n",
    "\n",
    "# Loop through all observations.\n",
    "for i in range(n):\n",
    "    # Subtract the current values of x and y from their\n",
    "    # respective averages, and multiply them. Add the\n",
    "    # outcome to the sum.\n",
    "    s += (x[i] - mean_x) * (y[i] - mean_y)\n",
    "\n",
    "print(\"Sum = {}\".format(round(s, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can write this a in a shorter way, by using NumPy's `sum` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = numpy.sum( (x - mean_x) * (y-mean_y) )\n",
    "\n",
    "print(\"Sum = {}\".format(round(s, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular sum, $\\Sigma^{n}_{i=1} (x_{i} - \\bar{x}) (y_{i} - \\bar{y})$, indicates how much x and y covary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part of the Pearson R equation was the bit below the line (the denominator), which includes $\\sqrt{\\Sigma^{n}_{i=1}(x_{i}-\\bar{x})^2}$. Let's work from the inside-out: We already know that $x_{i}-\\bar{x}$ means \"subtract the average value of x from the current value of x\". Here, that value is then squared: $(x_{i}-\\bar{x})^2$. All of these squares are then summed together (sum of squares!). Finally, the square root of this sum is calculated.\n",
    "\n",
    "You might recognise this value as the *variance*.\n",
    "\n",
    "Let's compute the values in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = (x - mean_x)**2\n",
    "sum_of_squares = numpy.sum(squares)\n",
    "variance = numpy.sqrt(sum_of_squares)\n",
    "\n",
    "print(\"Variance = {}\".format(round(variance, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write this in a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_x = numpy.sqrt(numpy.sum((x-mean_x)**2))\n",
    "\n",
    "print(\"Variance = {}\".format(round(var_x, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining what you learned above, you can compute the Pearson correlation of two values yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take Taylor Swift listening as 'x' values.\n",
    "x = tay_minutes\n",
    "mean_x = numpy.mean(x)\n",
    "# Let's take the happiness ratings of Year 3 undergrads \n",
    "# as 'y' values.\n",
    "y = happy_Y3\n",
    "mean_y = numpy.mean(y)\n",
    "\n",
    "# Compute the covariance of x and y.\n",
    "cov = numpy.sum( (x - mean_x) * (y-mean_y) )\n",
    "# Compute the variance of x.\n",
    "var_x = numpy.sqrt(numpy.sum((x-mean_x)**2))\n",
    "# Compute the variance of y.\n",
    "var_y = numpy.sqrt(numpy.sum((y-mean_y)**2))\n",
    "\n",
    "# Compute Pearson's R!\n",
    "r = cov / (var_x * var_y)\n",
    "\n",
    "print(\"Pearson's R = {}\".format(round(r, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final check to see if this matches what SciPy's Pearson R function produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = pearsonr(x,y)\n",
    "\n",
    "print(\"Pearson's R = {}\".format(round(r, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woop! You can now use and manually compute a Pearson correlation coefficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric correlation coefficients\n",
    "\n",
    "Pearson's correlation is what is referred to as a *parametric test*. This is because it assumes that the values in both variables (x and y) were drawn from a normal distribution. In practice, this won't always hold true.\n",
    "\n",
    "### Spearman's Rho\n",
    "\n",
    "If you want to compute a correlation, but your data are *not* normally distributed, the closest alternative might just be Spearman's $\\rho$ (Greek letter \"rho\"). Not unlike [the honey badger](https://www.youtube.com/watch?v=4r7wHMg5Yjg), Spearman doesn't care about how your data is distributed. Instead, it simply rank-orders all the values in both variables.\n",
    "\n",
    "Spearman's $\\rho$ thus depends on the covariance of the ranked variables x and y, not on their actual values. The equation is:\n",
    "\n",
    "$\\rho = {{cov(rg_{x}, rg_{y})} \\over {\\sigma_{rg_{x}} \\sigma_{rg_{y}}}}$\n",
    "\n",
    "Here, $rg_{x}$ is simply the rank-score of each value in the variable x. For example, if $x = [2, 5, 3]$ then $rg_{x} = [1, 3, 2]$. $\\sigma_{rg_{x}}$ is the standard deviation of the rank-scored values.\n",
    "\n",
    "**Important note**: the covariance in this equation is treated subtly differently from the covariance in the equation for Pearson's correlation coefficient. Specifically, the value is computed in exactly the same way, but for Spearman's $\\rho$ it is divided by the number of observations. A similar thing is true for the standard deviation, which is used here instead of the variance used for the Pearson's R computation above.\n",
    "\n",
    "Let's see Spearman's $\\rho$ in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a function that can rank-score our data.\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# Let's take Taylor Swift listening as 'x' values.\n",
    "x = rankdata(tay_minutes)\n",
    "mean_x = numpy.mean(x)\n",
    "# Let's take the happiness ratings of Year 3 undergrads\n",
    "# as 'y' values.\n",
    "y = rankdata(happy_Y3)\n",
    "mean_y = numpy.mean(y)\n",
    "\n",
    "# Count the number of observations, and save it as a \n",
    "# floating point number.\n",
    "n = float(len(x))\n",
    "\n",
    "# Compute the covariance of the ranked x and y.\n",
    "cov = numpy.sum( (x - mean_x) * (y-mean_y) ) / n\n",
    "# Compute the standard deviations of the ranked x and y.\n",
    "sd_x = numpy.std(x)\n",
    "sd_y = numpy.std(y)\n",
    "\n",
    "# Compute rho\n",
    "rho = cov / (sd_x * sd_y)\n",
    "\n",
    "print(\"Spearman's rho = {}\".format(round(rho, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can simply refer to a pre-existing function in SciPy's `stats` sub-module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spearmanr function from SciPy\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Let's take Taylor Swift listening as 'x' values.\n",
    "x = tay_minutes\n",
    "mean_x = numpy.mean(x)\n",
    "# Let's take the happiness ratings of Year 3 undergrads\n",
    "# as 'y' values.\n",
    "y = happy_Y3\n",
    "mean_y = numpy.mean(y)\n",
    "\n",
    "# Compute the Spearman rank correlation coefficient.\n",
    "rho, p = spearmanr(x, y)\n",
    "\n",
    "print(\"Spearman's rho = {}\".format(round(rho, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kendall's tau\n",
    "\n",
    "Conceptually, Spearman's $\\rho$ maps onto Pearson's $r$ very nicely. However, it is not the only option you have to compute a non-parametric correlation coefficient. There is also Kendall's $\\tau$. After counting all concordant (values differ from mean in same direction) and disconcordant (values differ from mean in opposite direction) pairs, Kendall's $\\tau$ can be computed like this:\n",
    "\n",
    "$\\tau = {{N_{concordant} - N_{disconcordant}} \\over {n (n-1)/2}}$\n",
    "\n",
    "Again, fortunately for us, SciPy has a function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Let's take Taylor Swift listening as 'x' values.\n",
    "x = tay_minutes\n",
    "# Let's take the happiness ratings of Year 3 undergrads\n",
    "# as 'y' values.\n",
    "y = happy_Y3\n",
    "\n",
    "# Compute Kendall's tau\n",
    "tau, p = kendalltau(x, y)\n",
    "\n",
    "print(\"Kendall's tau = {}\".format(round(tau, ndigits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the value for Kendall's $\\tau$ is lower than Spearman's $\\rho$ and Pearson's $r$, that does not mean it doesn't count as much. In fact, lower values of Kendall's $\\tau$ correspond to higher values of Spearman's $\\rho$ or Pearson's $r$. (For example, a Pearson R of 0.8 is equivalent to a Kendall $\\tau$ of 0.6)\n",
    "\n",
    "If you want to know more about why Kendall's $\\tau$ is more powerful than the other two correlation coefficients, please read the following article:\n",
    "\n",
    "- Bonett, D. G., & Wright, T. A. (2000). Sample size requirements for estimating Pearson, Kendall, and Spearman correlations. *Psychometrika*, *65*(1), 2328."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
